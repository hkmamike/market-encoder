# -*- coding: utf-8 -*-
"""A_sen_embed_transformer_v7.3_with_backtest_keep.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VC-8ttehH1TuEy-BbRL5QWIsIypsydkY

I have generated a plan to modify the notebook to use FinBERT embeddings instead of scalar sentiment scores. This involves extracting the embeddings, updating the data pipeline to handle high-dimensional vectors, and modifying the model architecture. I will also include a setup for co-finetuning FinBERT, though I'll add checks for memory constraints given the sequence length.

Install Dependencies: Install the transformers library to load the FinBERT model and tokenizer.
Extract FinBERT Embeddings: Load the pre-trained FinBERT model. Process the df_news dataframe to generate 768-dimensional embeddings for each article. Aggregate these embeddings by stock and date (e.g., using mean pooling) to create a daily sentiment vector.
Update Data Processing: Modify the merging logic to join the 768-dimensional sentiment vectors with the stock price data. Update the impute_decay function to handle vector imputation instead of scalar values. Scale the price features while keeping embeddings (or normalizing them separately) and create time-series sequences.
Define Transformer Model (Frozen Embeddings): Update the TimeSeriesTransformer class to accept the new input dimension (3 price features + 768 embedding features). This model will use the pre-computed frozen embeddings.
Define Co-Finetuning Model Architecture: Create a new model class FinBERT_TimeSeries_Model that integrates the FinBERT module directly. This class will accept tokenized text inputs and price sequences, allowing for end-to-end fine-tuning of FinBERT weights. (Note: This will be structured to allow toggling between frozen and unfrozen states).
Train and Evaluate: Set up the training loop to train the modified transformer using the frozen embeddings. Evaluate the performance on the test set and visualize the predictions.
Final Task: Summarize the changes made to the architecture and the results of using embeddings versus the original scalar approach.

# Setup

## Install
"""

!pip install awswrangler

"""## Import"""

import pandas as pd
import awswrangler as wr
import boto3
import numpy as np
import matplotlib.pyplot as plt
from getpass import getpass
import os

import torch
import torch.nn as nn
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import DataLoader, Dataset

"""## Constants"""

# --- IMPORTANT: Set these variables before running ---
AWS_REGION = 'us-east-2'
S3_STAGING_DIR = 's3://cs230-market-data-2025/athena-query-results/'
ATHENA_DB = 'cs230_finance_data'
# Querying more data for a small training run
# SQL_QUERY = "SELECT concatarticles1, concatarticles2, vol_1_vs_2 FROM paired_vixy_w_titles_v3 WHERE vol_1_vs_2 is not null ORDER BY RAND() LIMIT 10000"
# SQL_QUERY = "SELECT concatarticles1, concatarticles2, open_lead_close_lag_change_pct_bucket1 as bucket1, open_lead_close_lag_change_pct_bucket2 as bucket2, samebucket_open_lead_close_lag_change_pct as samebucket FROM paired_vixy_w_titles_dedupped ORDER BY RAND() LIMIT 10000"
SQL_QUERY_NEWS = "SELECT * FROM fnspid_nasdaq_news_top5_summaries"
SQL_QUERY_STOCKS = "SELECT * FROM fnspid_stock_prices_top5"

SEQ_LEN = 50 # Small for this dummy data, use 30 or 60 for real usage
FUTURE_LEN = 3

# ----------------------------------------------------

"""## Connect AWS"""

# --- AWS Authentication for Colab ---
# Prompt for AWS credentials
aws_access_key_id = getpass('Enter AWS Access Key ID: ')
aws_secret_access_key = getpass('Enter AWS Secret Access Key: ')

"""# Preprocessing

## Read data from aws
"""

print(f"\n--- Step 3: Configuration set for {ATHENA_DB} ---")
print(f"--- Step 4: Querying Data ---")
print(f"Querying data from {ATHENA_DB}....")

# Define df in a wider scope
df_news = None
df_stocks = None

try:
    # Create a boto3 session with the provided credentials
    session = boto3.Session(
        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key,
        region_name=AWS_REGION,
    )

    # Run the query and load results into a Pandas DataFrame
    df_news = wr.athena.read_sql_query(
        sql=SQL_QUERY_NEWS,
        database=ATHENA_DB,
        s3_output=S3_STAGING_DIR,
        boto3_session=session,
    )

    df_stocks = wr.athena.read_sql_query(
        sql=SQL_QUERY_STOCKS,
        database=ATHENA_DB,
        s3_output=S3_STAGING_DIR,
        boto3_session=session,
    )
    SQL_QUERY_STOCKS

    print("\nQuery successful! Data loaded into DataFrame.")



except Exception as e:
    print(f"\nAn error occurred:")
    print(e)

df_news

"""## Process stock price"""

df = df_stocks.copy()
df['date'] = pd.to_datetime(df['date']).dt.normalize()

# Helper function to create sequences
def create_sequences(features, target, lookback=50, horizon=3):
    X, y = [], []
    # We create sequences of length 'lookback'
    # The target is 'horizon' steps after the sequence ends
    # Sequence ends at index i + lookback - 1
    # Target index is (i + lookback - 1) + horizon
    for i in range(len(features) - lookback - horizon + 1):
        X.append(features[i : i + lookback])
        y.append(target[i + lookback + horizon - 1])
    return np.array(X), np.array(y)

print("Helper function 'create_sequences' defined.")
# The actual data generation loop has been moved to the Sentiment Fusion section
# to avoid doing the work twice.

"""# Task
Install the `transformers` library. Then, load the pre-trained FinBERT model and tokenizer. Process the `df_news` DataFrame to generate 768-dimensional embeddings for each article using the 'lsa_summary' column. Finally, aggregate these embeddings by stock and date (e.g., using mean pooling) to create a daily sentiment vector.

## Install Dependencies

### Subtask:
Install the `transformers` library to load the FinBERT model and tokenizer.

**Reasoning**:
Install the transformers library to enable loading the FinBERT model and tokenizer.
"""

!pip install transformers

"""## Extract FinBERT Embeddings

### Subtask:
Generate 768-dimensional embeddings for the news summaries using the pre-trained FinBERT model and aggregate them by stock and date.

**Reasoning**:
Implement the embedding extraction process using FinBERT. This involves loading the model, processing the news summaries in batches to extract CLS token embeddings, and aggregating them by stock and date. This directly follows the provided instructions.
"""

import torch
import pandas as pd
import numpy as np
from tqdm import tqdm
from transformers import BertTokenizer, BertModel

# # 1. Setup Device
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# print(f"Using device: {device}")

# # 2. Load Model and Tokenizer
# MODEL_NAME = "ProsusAI/finbert"
# tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)
# model = BertModel.from_pretrained(MODEL_NAME)
# model.to(device)
# model.eval()

# # 3. Data Preparation
# # Check if df_news exists in global scope
# if 'df_news' not in globals():
#     raise ValueError("df_news is not defined. Please ensure the data loading step was successful.")

# summaries = df_news['lsa_summary'].fillna("").astype(str).tolist()

# # 4. Generate Embeddings
# all_embeddings = []
# batch_size = 32

# print(f"Generating embeddings for {len(summaries)} articles...")

# for i in tqdm(range(0, len(summaries), batch_size)):
#     batch_texts = summaries[i : i + batch_size]

#     # Tokenize
#     inputs = tokenizer(
#         batch_texts,
#         return_tensors="pt",
#         padding=True,
#         truncation=True,
#         max_length=256
#     ).to(device)

#     # Inference
#     with torch.no_grad():
#         outputs = model(**inputs)
#         # CLS token is at index 0
#         cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
#         all_embeddings.append(cls_embeddings)

# # 5. Aggregate
# if all_embeddings:
#     embeddings_matrix = np.vstack(all_embeddings)

#     # Create DataFrame
#     embedding_df = pd.DataFrame(embeddings_matrix)

#     # Add metadata
#     embedding_df['stock_symbol'] = df_news['stock_symbol'].values

#     # Handle date column name variations
#     date_col = 'timestamp' if 'timestamp' in df_news.columns else 'date'
#     embedding_df['date'] = pd.to_datetime(df_news[date_col]).dt.normalize()

#     # Group by stock and date
#     print("Aggregating embeddings...")
#     sentiment_embeddings = embedding_df.groupby(['stock_symbol', 'date']).mean().reset_index()

#     print(f"Sentiment embeddings shape: {sentiment_embeddings.shape}")
#     print(sentiment_embeddings.head())

# else:
#     print("No embeddings generated.")

"""# Task
Save the generated `sentiment_embeddings` DataFrame to a CSV file named "sentiment_embeddings.csv" for persistence. Then, merge these embeddings with the `df_stocks` DataFrame based on stock symbol and date. Update the data processing pipeline to handle the 768-dimensional sentiment vectors by implementing an imputation strategy for missing days (e.g., forward fill or decay) and scaling the numerical stock price features (Open, Close, Volume) while keeping the embeddings unchanged. Finally, generate the time-series sequences and split the data into training and testing sets.

## Save and Load Embeddings

### Subtask:
Save the generated embeddings to a CSV file for persistence and implement loading logic.

**Reasoning**:
I will implement the logic to save the `sentiment_embeddings` dataframe to a CSV file if it exists in memory, or load it from the file if it doesn't, ensuring the 'date' column is properly converted to datetime objects.
"""

import os
import pandas as pd

# Check if sentiment_embeddings exists in memory
if 'sentiment_embeddings' in globals():
    print("Saving embeddings to CSV...")
    sentiment_embeddings.to_csv('sentiment_embeddings.csv', index=False)
    print("Embeddings saved to 'sentiment_embeddings.csv'.")
else:
    print("Variable 'sentiment_embeddings' not found in memory. Checking for file...")
    if os.path.exists('sentiment_embeddings.csv'):
        print("Loading embeddings from 'sentiment_embeddings.csv'...")
        sentiment_embeddings = pd.read_csv('sentiment_embeddings.csv')
        sentiment_embeddings['date'] = pd.to_datetime(sentiment_embeddings['date'])
        print("Embeddings loaded successfully.")
    else:
        print("Error: 'sentiment_embeddings.csv' not found. Please run the embedding generation step first.")

# Verify
if 'sentiment_embeddings' in globals():
    print(f"Sentiment Embeddings Shape: {sentiment_embeddings.shape}")
    print(sentiment_embeddings.head())

"""**Reasoning**:
I will merge the stock data with the generated sentiment embeddings, using a forward fill strategy to impute missing sentiment values as permitted by the instructions. Then, I will scale the numerical stock features and generate time-series sequences for training and testing.


"""

import numpy as np
from sklearn.preprocessing import MinMaxScaler

lookback = 50
horizon = 3

# 1. Prepare Embeddings
# Rename columns to ensure they are distinct strings and avoid conflict
embedding_cols = [col for col in sentiment_embeddings.columns if col not in ['stock_symbol', 'date']]
new_col_names = {col: f"emb_{col}" for col in embedding_cols}
sentiment_embeddings_renamed = sentiment_embeddings.rename(columns=new_col_names)
embedding_feature_names = list(new_col_names.values())

# 2. Merge with Stock Data
# Ensure 'date' column in df is datetime and normalized
df['date'] = pd.to_datetime(df['date']).dt.normalize()

# Merge on stock_symbol and date (Left Join to keep all stock data)
merged_df = pd.merge(df, sentiment_embeddings_renamed, on=['stock_symbol', 'date'], how='left')

# 3. Impute Missing Embeddings
print("Imputing missing embeddings (Forward Fill then Zero Fill)...")
# Forward fill embeddings per stock to persist sentiment
merged_df[embedding_feature_names] = merged_df.groupby('stock_symbol')[embedding_feature_names].ffill()
# Fill remaining NaNs (e.g., beginning of history) with 0.0
merged_df[embedding_feature_names] = merged_df[embedding_feature_names].fillna(0.0)

# 4. Setup for Sequence Generation
print("Generating sequences with strict Train/Test split...")
X_train_all, y_train_all = [], []
X_test_all, y_test_all = [], []
scalers = {}

price_features = ['open', 'close', 'volume']

for symbol, group in merged_df.groupby('stock_symbol'):
    # Sort by date just in case
    group = group.sort_values('date')

    # a. Extract features
    # Price features (to be scaled)
    raw_prices = group[price_features].values
    # Embedding features (already normalized/frozen, we keep them as is)
    raw_embeddings = group[embedding_feature_names].values

    # Skip if too short
    if len(raw_prices) < (lookback + horizon + 1):
        continue

    # b. Split raw data (80% Train, 20% Test)
    split_idx = int(len(raw_prices) * 0.8)

    train_prices = raw_prices[:split_idx]
    test_prices = raw_prices[split_idx:]

    train_embeddings = raw_embeddings[:split_idx]
    test_embeddings = raw_embeddings[split_idx:]

    # c. Scaling (Fit on Train, Transform on Test)
    scaler = MinMaxScaler()
    train_prices_scaled = scaler.fit_transform(train_prices)
    test_prices_scaled = scaler.transform(test_prices)

    # Store scaler
    scalers[symbol] = scaler

    # d. Combine Features (Prices + Embeddings)
    # We assume embeddings are features, not targets
    train_features = np.hstack([train_prices_scaled, train_embeddings])
    test_features = np.hstack([test_prices_scaled, test_embeddings])

    # e. Define Targets (Predicting 'Close' price, which is index 1 in price_features)
    train_target = train_prices_scaled[:, 1]
    test_target = test_prices_scaled[:, 1]

    # f. Create Sequences
    X_tr, y_tr = create_sequences(train_features, train_target, lookback, horizon)
    X_te, y_te = create_sequences(test_features, test_target, lookback, horizon)

    if len(X_tr) > 0:
        X_train_all.append(X_tr)
        y_train_all.append(y_tr)
    if len(X_te) > 0:
        X_test_all.append(X_te)
        y_test_all.append(y_te)

# 5. Concatenate
if X_train_all:
    X_train = np.concatenate(X_train_all, axis=0)
    y_train = np.concatenate(y_train_all, axis=0)
    X_test = np.concatenate(X_test_all, axis=0)
    y_test = np.concatenate(y_test_all, axis=0)
else:
    raise ValueError("No sequences generated. Check data length and sequence parameters.")

print(f"\nProcessing Complete.")
print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")
print(f"Input Feature Dimension: {X_train.shape[2]}")

"""# Task
Read the content of "/content/cleaned_a_sen_transformer.py" to identify and extract the specific 4-layer vanilla transformer class definition. Then, based on this architecture, redefine the `TimeSeriesTransformer` class to accept the 771-dimensional input (frozen embeddings + price features) and creating a new `FinBERT_TimeSeries_Model` class that integrates the FinBERT module for co-finetuning.
"""

import torch
import torch.nn as nn
import math
import torch.compiler._cache
# from transformers import BertModel #uncomment

# --- PATCH START ---
# Fix for "Artifact of type=pgo already registered" error in Colab/Torch
def safe_register(cls, artifact_cls):
    artifact_type_key = artifact_cls.type()
    if artifact_type_key in cls._artifact_types:
        return artifact_cls
    # Original logic without the assertion failure
    cls._artifact_types[artifact_type_key] = artifact_cls
    return artifact_cls

torch.compiler._cache.CacheArtifactFactory.register = classmethod(safe_register)
# --- PATCH END ---

# 1. Positional Encoding (Helper)
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x shape: (batch, seq_len, d_model)
        return x + self.pe[:, :x.size(1), :]

# 2. Time Series Transformer (Updated for Frozen Embeddings)
class TimeSeriesTransformer(nn.Module):
    def __init__(self, input_dim, d_model, num_heads, num_layers, output_dim, dropout=0.2, dim_feedforward=128):
        super(TimeSeriesTransformer, self).__init__()
        self.d_model = d_model

        # Input Projection
        self.input_projection = nn.Linear(input_dim, d_model)
        self.activation = nn.ReLU()

        # Positional Encoding
        self.pos_encoder = PositionalEncoding(d_model)
        self.pos_dropout = nn.Dropout(dropout)

        # Transformer Encoder
        # Pre-LN configuration
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=num_heads,
            batch_first=True,
            dropout=dropout,
            dim_feedforward=dim_feedforward,
            norm_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # Decoder / Output Layer
        self.decoder = nn.Linear(d_model, output_dim)

    def forward(self, x):
        # x shape: (batch, seq_len, input_dim)

        # Project and Scale
        x = self.input_projection(x) * math.sqrt(self.d_model)
        x = self.activation(x)

        # Add Positional Encoding
        x = self.pos_encoder(x)
        x = self.pos_dropout(x)

        # Transformer Encoder
        x = self.transformer_encoder(x)

        # Select last time step
        x = x[:, -1, :]

        # Output
        x = self.decoder(x)
        return x

# 3. Co-Finetuning Model Architecture
class FinBERT_TimeSeries_Model(nn.Module):
    def __init__(self, price_dim, d_model, num_heads, num_layers, output_dim, dropout=0.2, dim_feedforward=128, freeze_finbert=True):
        super(FinBERT_TimeSeries_Model, self).__init__()

        # Load Pre-trained FinBERT
        self.finbert = BertModel.from_pretrained("ProsusAI/finbert")

        # Toggle Freezing
        if freeze_finbert:
            for param in self.finbert.parameters():
                param.requires_grad = False

        # FinBERT embedding size is 768
        self.embedding_dim = 768
        combined_input_dim = price_dim + self.embedding_dim

        # Initialize the TimeSeriesTransformer component
        self.transformer = TimeSeriesTransformer(
            input_dim=combined_input_dim,
            d_model=d_model,
            num_heads=num_heads,
            num_layers=num_layers,
            output_dim=output_dim,
            dropout=dropout,
            dim_feedforward=dim_feedforward
        )

    def forward(self, price_seq, input_ids, attention_mask):
        # price_seq: (batch, seq_len, price_dim)
        # input_ids: (batch, seq_len, token_len)
        # attention_mask: (batch, seq_len, token_len)

        batch_size, seq_len, _ = price_seq.shape

        # Flatten for FinBERT processing: (batch * seq_len, token_len)
        input_ids_flat = input_ids.view(-1, input_ids.size(-1))
        attention_mask_flat = attention_mask.view(-1, attention_mask.size(-1))

        # Get FinBERT Embeddings
        # We use the CLS token (index 0) from the last hidden state
        outputs = self.finbert(input_ids=input_ids_flat, attention_mask=attention_mask_flat)
        cls_embeddings = outputs.last_hidden_state[:, 0, :] # Shape: (batch * seq_len, 768)

        # Reshape back to sequence format: (batch, seq_len, 768)
        text_embeddings = cls_embeddings.view(batch_size, seq_len, self.embedding_dim)

        # Concatenate Price Features and Text Embeddings
        # Result Shape: (batch, seq_len, price_dim + 768)
        combined_features = torch.cat((price_seq, text_embeddings), dim=2)

        # Pass through Transformer
        output = self.transformer(combined_features)

        return output

print("Model classes 'TimeSeriesTransformer' and 'FinBERT_TimeSeries_Model' defined successfully.")

"""## Train and Evaluate (Frozen)

### Subtask:
Train the `TimeSeriesTransformer` using the pre-computed frozen embeddings and evaluate its performance.

**Reasoning**:
Implement the training and evaluation pipeline for the TimeSeriesTransformer model using the pre-computed embeddings. This includes data conversion to tensors, creating DataLoaders, model instantiation, the training loop, metric calculation, and visualization of results.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import sys

# --- Fix for 'partially initialized module torch._dynamo' error ---
# Clear any corrupted dynamo modules from sys.modules to force clean re-import
modules_to_remove = [m for m in sys.modules if m.startswith("torch._dynamo")]
for m in modules_to_remove:
    del sys.modules[m]
# ------------------------------------------------------------------

# 1. Convert to PyTorch Tensors
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Ensure data is float32
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Shape (N, 1)

X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)    # Shape (N, 1)

# 2. Create DataLoaders
BATCH_SIZE = 64
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

# 3. Instantiate Model
input_dim = X_train.shape[2]  # Should be 771
d_model = 32
num_heads = 4
num_layers = 4
output_dim = 1

# Ensure TimeSeriesTransformer is defined (from previous cell)
if 'TimeSeriesTransformer' not in globals():
    raise NameError("TimeSeriesTransformer class is not defined. Please run the previous cell.")

model = TimeSeriesTransformer(
    input_dim=input_dim,
    d_model=d_model,
    num_heads=num_heads,
    num_layers=num_layers,
    output_dim=output_dim,
    dropout=0.2
).to(device)

# 4. Loss and Optimizer
criterion = nn.MSELoss()
optimizer = optim.AdamW(model.parameters(), lr=1e-3)

# 5. Training Loop
num_epochs = 100
train_losses = []
test_losses = []

print(f"Starting training for {num_epochs} epochs...")

for epoch in range(num_epochs):
    model.train()
    running_train_loss = 0.0

    for X_batch, y_batch in train_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)

        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()

        running_train_loss += loss.item() * X_batch.size(0)

    epoch_train_loss = running_train_loss / len(train_loader.dataset)
    train_losses.append(epoch_train_loss)

    # Evaluation Phase
    model.eval()
    running_test_loss = 0.0
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            running_test_loss += loss.item() * X_batch.size(0)

    epoch_test_loss = running_test_loss / len(test_loader.dataset)
    test_losses.append(epoch_test_loss)

    print(f"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {epoch_train_loss:.6f} | Test Loss: {epoch_test_loss:.6f}")

# 6. Evaluation Metrics
model.eval()
with torch.no_grad():
    test_predictions = []
    test_targets = []
    last_known_prices = [] # Store last known price for directional accuracy

    for X_batch, y_batch in test_loader:
        X_batch = X_batch.to(device)
        preds = model(X_batch).cpu().numpy()
        test_predictions.extend(preds)
        test_targets.extend(y_batch.numpy())

        # Extract the last known 'Close' price from the input sequence
        # Feature index 1 corresponds to 'Close' in our preprocessing
        # X_batch shape: (batch, seq_len, features)
        last_known_prices.extend(X_batch[:, -1, 1].cpu().numpy())

test_predictions = np.array(test_predictions).flatten()
test_targets = np.array(test_targets).flatten()
last_known_prices = np.array(last_known_prices).flatten()

mse = mean_squared_error(test_targets, test_predictions)
mae = mean_absolute_error(test_targets, test_predictions)
r2 = r2_score(test_targets, test_predictions)

# Directional Accuracy Calculation
# Calculate movement: Target - Last_Known_Price
actual_moves = np.sign(test_targets - last_known_prices)
pred_moves = np.sign(test_predictions - last_known_prices)

# Accuracy: Fraction of times the predicted direction matches the actual direction
dir_accuracy = np.mean(actual_moves == pred_moves)

print("\nFinal Evaluation Metrics:")
print(f"Mean Squared Error (MSE): {mse:.6f}")
print(f"Mean Absolute Error (MAE): {mae:.6f}")
print(f"R2 Score: {r2:.6f}")
print(f"Directional Accuracy: {dir_accuracy:.2%}")

# 7. Plot Results (Subset for clarity)
plt.figure(figsize=(12, 6))
plt.plot(test_targets[:100], label='Actual', alpha=0.7)
plt.plot(test_predictions[:100], label='Predicted', alpha=0.7)
plt.title('Actual vs Predicted (First 100 Test Samples)')
plt.xlabel('Sample Index')
plt.ylabel('Scaled Price')
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Parameters
H = horizon  # 3

# 1. Align Predictions (Compare Pred[t] vs Pred[t-H])
# We lose the first H data points because we don't have a 'previous prediction' for them.
pred_future = test_predictions[H:]      # Prediction for T+3 (made at T)
pred_past   = test_predictions[:-H]     # Prediction for T   (made at T-3)

# 2. Generate Signal
# If Future Prediction > Past Prediction -> Model expects growth relative to its own baseline
relative_signal = np.sign(pred_future - pred_past)

# 3. Align Actual Moves
# We need the actual market moves corresponding to 'pred_future'
# actual_diff is (Target - Last_Known), aligned with test_predictions.
actual_moves_aligned = actual_diff[H:]

# 4. Calculate Returns
# Market Baseline (Buy and Hold) for this trimmed period
market_pnl_trimmed = actual_moves_aligned

# Strategy PnL
strategy_pnl_relative = relative_signal * actual_moves_aligned

# 5. Metrics
total_market_return_trim = np.sum(market_pnl_trimmed)
total_strategy_return_rel = np.sum(strategy_pnl_relative)

# Directional Accuracy of this new signal
# Check if the relative signal matched the actual market direction
actual_direction_trimmed = np.sign(actual_moves_aligned)
new_dir_accuracy = np.mean(relative_signal == actual_direction_trimmed)

print(f"--- De-Biased 'Relative Delta' Strategy ---")
print(f"Total Market Return (Trimmed): {total_market_return_trim:.4f}")
print(f"Total Strategy Return (Relative): {total_strategy_return_rel:.4f}")
print(f"New Directional Accuracy: {new_dir_accuracy:.2%}")

if total_strategy_return_rel > total_market_return_trim:
    print("\nRESULT: SUCCESS! The relative strategy OUTPERFORMS the market. The model has hidden alpha!")
else:
    print("\nRESULT: Still underperforming. The model might just be outputting noise.")

# 6. Visualize
plt.figure(figsize=(12, 6))
plt.plot(np.cumsum(market_pnl_trimmed), label='Market Baseline', color='gray', alpha=0.5)
plt.plot(np.cumsum(strategy_pnl_relative), label='Relative Delta Strategy', color='green')
plt.title(f'De-Biased Strategy (Comparing Pred[t+{H}] vs Pred[t])')
plt.xlabel('Test Samples')
plt.ylabel('Cumulative Return')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()