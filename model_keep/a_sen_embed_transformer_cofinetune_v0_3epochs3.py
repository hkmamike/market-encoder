# -*- coding: utf-8 -*-
"""A_sen_embed_transformer_cofinetune_v0_3epochs3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CaWdmqHuzG7SRz2N3shVU9JLPkxh7ROv

I've updated the plan to include specific constraints for the co-finetuning process. We will selectively unfreeze only the last two layers of FinBERT to preserve its pre-trained knowledge while adapting it to the task. Additionally, we will implement a differential learning rate strategy during training, applying a smaller learning rate to the FinBERT components to ensure stable convergence.

Prepare Data for Co-Finetuning: Aggregate news summaries by stock and date, merge them with the stock price data, and impute missing text. Tokenize the aggregated text using the FinBERT tokenizer and scale the stock numerical features (Open, Close, Volume) to prepare inputs for the model.
Create Custom Dataset and DataLoaders: Define a TextStockDataset class to manage the multimodal data (stock prices and tokenized text sequences). Create train_loader and test_loader with an appropriate batch size to handle the memory requirements.
Initialize Model with Partial Unfreezing: Instantiate the FinBERT_TimeSeries_Model. Configure the model to freeze the majority of the FinBERT parameters, specifically unfreezing only the last 2 transformer layers to allow for targeted fine-tuning.
Train with Differential Learning Rates: Configure the optimizer with parameter groups to apply a small learning rate (e.g., 1e-5) to the unfrozen FinBERT layers and a standard learning rate (e.g., 1e-3) to the TimeSeriesTransformer. Train the model end-to-end and evaluate performance.
Final Task: Summarize the changes made for co-finetuning and the resulting model performance.

# Setup

## Install
"""

!pip install awswrangler transformers

"""## Import"""

import pandas as pd
import awswrangler as wr
import boto3
import numpy as np
import matplotlib.pyplot as plt
from getpass import getpass
import os

import torch
import torch.nn as nn
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import DataLoader, Dataset

"""## Constants"""

# --- IMPORTANT: Set these variables before running ---
AWS_REGION = 'us-east-2'
S3_STAGING_DIR = 's3://cs230-market-data-2025/athena-query-results/'
ATHENA_DB = 'cs230_finance_data'
# Querying more data for a small training run
# SQL_QUERY = "SELECT concatarticles1, concatarticles2, vol_1_vs_2 FROM paired_vixy_w_titles_v3 WHERE vol_1_vs_2 is not null ORDER BY RAND() LIMIT 10000"
# SQL_QUERY = "SELECT concatarticles1, concatarticles2, open_lead_close_lag_change_pct_bucket1 as bucket1, open_lead_close_lag_change_pct_bucket2 as bucket2, samebucket_open_lead_close_lag_change_pct as samebucket FROM paired_vixy_w_titles_dedupped ORDER BY RAND() LIMIT 10000"
SQL_QUERY_NEWS = "SELECT * FROM fnspid_nasdaq_news_top5_summaries"
SQL_QUERY_STOCKS = "SELECT * FROM fnspid_stock_prices_top5"

SEQ_LEN = 50 # Small for this dummy data, use 30 or 60 for real usage
FUTURE_LEN = 3

# ----------------------------------------------------

"""## Connect AWS"""

# --- AWS Authentication for Colab ---
# Prompt for AWS credentials
aws_access_key_id = getpass('Enter AWS Access Key ID: ')
aws_secret_access_key = getpass('Enter AWS Secret Access Key: ')

"""# Preprocessing

## Read data from aws
"""

print(f"\n--- Step 3: Configuration set for {ATHENA_DB} ---")
print(f"--- Step 4: Querying Data ---")
print(f"Querying data from {ATHENA_DB}....")

# Define df in a wider scope
df_news = None
df_stocks = None

try:
    # Create a boto3 session with the provided credentials
    session = boto3.Session(
        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key,
        region_name=AWS_REGION,
    )

    # Run the query and load results into a Pandas DataFrame
    df_news = wr.athena.read_sql_query(
        sql=SQL_QUERY_NEWS,
        database=ATHENA_DB,
        s3_output=S3_STAGING_DIR,
        boto3_session=session,
    )

    df_stocks = wr.athena.read_sql_query(
        sql=SQL_QUERY_STOCKS,
        database=ATHENA_DB,
        s3_output=S3_STAGING_DIR,
        boto3_session=session,
    )
    SQL_QUERY_STOCKS

    print("\nQuery successful! Data loaded into DataFrame.")



except Exception as e:
    print(f"\nAn error occurred:")
    print(e)

df_news

"""## Process stock price"""

df = df_stocks.copy()
df['date'] = pd.to_datetime(df['date']).dt.normalize()

# Helper function to create sequences
def create_sequences(features, target, lookback=50, horizon=3):
    X, y = [], []
    # We create sequences of length 'lookback'
    # The target is 'horizon' steps after the sequence ends
    # Sequence ends at index i + lookback - 1
    # Target index is (i + lookback - 1) + horizon
    for i in range(len(features) - lookback - horizon + 1):
        X.append(features[i : i + lookback])
        y.append(target[i + lookback + horizon - 1])
    return np.array(X), np.array(y)

print("Helper function 'create_sequences' defined.")
# The actual data generation loop has been moved to the Sentiment Fusion section
# to avoid doing the work twice.

"""## Extract FinBERT Embeddings

### Subtask:
Generate 768-dimensional embeddings for the news summaries using the pre-trained FinBERT model and aggregate them by stock and date.

**Reasoning**:
Implement the embedding extraction process using FinBERT. This involves loading the model, processing the news summaries in batches to extract CLS token embeddings, and aggregating them by stock and date. This directly follows the provided instructions.
"""

import torch
import pandas as pd
import numpy as np
from tqdm import tqdm
from transformers import BertTokenizer, BertModel

# # 1. Setup Device
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# print(f"Using device: {device}")

# # 2. Load Model and Tokenizer
# MODEL_NAME = "ProsusAI/finbert"
# tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)
# model = BertModel.from_pretrained(MODEL_NAME)
# model.to(device)
# model.eval()

# # 3. Data Preparation
# # Check if df_news exists in global scope
# if 'df_news' not in globals():
#     raise ValueError("df_news is not defined. Please ensure the data loading step was successful.")

# summaries = df_news['lsa_summary'].fillna("").astype(str).tolist()

# # 4. Generate Embeddings
# all_embeddings = []
# batch_size = 32

# print(f"Generating embeddings for {len(summaries)} articles...")

# for i in tqdm(range(0, len(summaries), batch_size)):
#     batch_texts = summaries[i : i + batch_size]

#     # Tokenize
#     inputs = tokenizer(
#         batch_texts,
#         return_tensors="pt",
#         padding=True,
#         truncation=True,
#         max_length=256
#     ).to(device)

#     # Inference
#     with torch.no_grad():
#         outputs = model(**inputs)
#         # CLS token is at index 0
#         cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
#         all_embeddings.append(cls_embeddings)

# # 5. Aggregate
# if all_embeddings:
#     embeddings_matrix = np.vstack(all_embeddings)

#     # Create DataFrame
#     embedding_df = pd.DataFrame(embeddings_matrix)

#     # Add metadata
#     embedding_df['stock_symbol'] = df_news['stock_symbol'].values

#     # Handle date column name variations
#     date_col = 'timestamp' if 'timestamp' in df_news.columns else 'date'
#     embedding_df['date'] = pd.to_datetime(df_news[date_col]).dt.normalize()

#     # Group by stock and date
#     print("Aggregating embeddings...")
#     sentiment_embeddings = embedding_df.groupby(['stock_symbol', 'date']).mean().reset_index()

#     print(f"Sentiment embeddings shape: {sentiment_embeddings.shape}")
#     print(sentiment_embeddings.head())

# else:
#     print("No embeddings generated.")

"""# Task
Save the generated `sentiment_embeddings` DataFrame to a CSV file named "sentiment_embeddings.csv" for persistence. Then, merge these embeddings with the `df_stocks` DataFrame based on stock symbol and date. Update the data processing pipeline to handle the 768-dimensional sentiment vectors by implementing an imputation strategy for missing days (e.g., forward fill or decay) and scaling the numerical stock price features (Open, Close, Volume) while keeping the embeddings unchanged. Finally, generate the time-series sequences and split the data into training and testing sets.

## Save and Load Embeddings

### Subtask:
Save the generated embeddings to a CSV file for persistence and implement loading logic.

**Reasoning**:
I will implement the logic to save the `sentiment_embeddings` dataframe to a CSV file if it exists in memory, or load it from the file if it doesn't, ensuring the 'date' column is properly converted to datetime objects.
"""

import os
import pandas as pd

# Check if sentiment_embeddings exists in memory
if 'sentiment_embeddings' in globals():
    print("Saving embeddings to CSV...")
    sentiment_embeddings.to_csv('sentiment_embeddings.csv', index=False)
    print("Embeddings saved to 'sentiment_embeddings.csv'.")
else:
    print("Variable 'sentiment_embeddings' not found in memory. Checking for file...")
    if os.path.exists('sentiment_embeddings.csv'):
        print("Loading embeddings from 'sentiment_embeddings.csv'...")
        sentiment_embeddings = pd.read_csv('sentiment_embeddings.csv')
        sentiment_embeddings['date'] = pd.to_datetime(sentiment_embeddings['date'])
        print("Embeddings loaded successfully.")
    else:
        print("Error: 'sentiment_embeddings.csv' not found. Please run the embedding generation step first.")

# Verify
if 'sentiment_embeddings' in globals():
    print(f"Sentiment Embeddings Shape: {sentiment_embeddings.shape}")
    print(sentiment_embeddings.head())

"""**Reasoning**:
I will merge the stock data with the generated sentiment embeddings, using a forward fill strategy to impute missing sentiment values as permitted by the instructions. Then, I will scale the numerical stock features and generate time-series sequences for training and testing.


"""

import numpy as np
from sklearn.preprocessing import MinMaxScaler

lookback = 50
horizon = 3

# 1. Prepare Embeddings
# Rename columns to ensure they are distinct strings and avoid conflict
embedding_cols = [col for col in sentiment_embeddings.columns if col not in ['stock_symbol', 'date']]
new_col_names = {col: f"emb_{col}" for col in embedding_cols}
sentiment_embeddings_renamed = sentiment_embeddings.rename(columns=new_col_names)
embedding_feature_names = list(new_col_names.values())

# 2. Merge with Stock Data
# Ensure 'date' column in df is datetime and normalized
df['date'] = pd.to_datetime(df['date']).dt.normalize()

# Merge on stock_symbol and date (Left Join to keep all stock data)
merged_df = pd.merge(df, sentiment_embeddings_renamed, on=['stock_symbol', 'date'], how='left')

# 3. Impute Missing Embeddings
print("Imputing missing embeddings (Forward Fill then Zero Fill)...")
# Forward fill embeddings per stock to persist sentiment
merged_df[embedding_feature_names] = merged_df.groupby('stock_symbol')[embedding_feature_names].ffill()
# Fill remaining NaNs (e.g., beginning of history) with 0.0
merged_df[embedding_feature_names] = merged_df[embedding_feature_names].fillna(0.0)

# 4. Setup for Sequence Generation
print("Generating sequences with strict Train/Test split...")
X_train_all, y_train_all = [], []
X_test_all, y_test_all = [], []
scalers = {}

price_features = ['open', 'close', 'volume']

for symbol, group in merged_df.groupby('stock_symbol'):
    # Sort by date just in case
    group = group.sort_values('date')

    # a. Extract features
    # Price features (to be scaled)
    raw_prices = group[price_features].values
    # Embedding features (already normalized/frozen, we keep them as is)
    raw_embeddings = group[embedding_feature_names].values

    # Skip if too short
    if len(raw_prices) < (lookback + horizon + 1):
        continue

    # b. Split raw data (80% Train, 20% Test)
    split_idx = int(len(raw_prices) * 0.8)

    train_prices = raw_prices[:split_idx]
    test_prices = raw_prices[split_idx:]

    train_embeddings = raw_embeddings[:split_idx]
    test_embeddings = raw_embeddings[split_idx:]

    # c. Scaling (Fit on Train, Transform on Test)
    scaler = MinMaxScaler()
    train_prices_scaled = scaler.fit_transform(train_prices)
    test_prices_scaled = scaler.transform(test_prices)

    # Store scaler
    scalers[symbol] = scaler

    # d. Combine Features (Prices + Embeddings)
    # We assume embeddings are features, not targets
    train_features = np.hstack([train_prices_scaled, train_embeddings])
    test_features = np.hstack([test_prices_scaled, test_embeddings])

    # e. Define Targets (Predicting 'Close' price, which is index 1 in price_features)
    train_target = train_prices_scaled[:, 1]
    test_target = test_prices_scaled[:, 1]

    # f. Create Sequences
    X_tr, y_tr = create_sequences(train_features, train_target, lookback, horizon)
    X_te, y_te = create_sequences(test_features, test_target, lookback, horizon)

    if len(X_tr) > 0:
        X_train_all.append(X_tr)
        y_train_all.append(y_tr)
    if len(X_te) > 0:
        X_test_all.append(X_te)
        y_test_all.append(y_te)

# 5. Concatenate
if X_train_all:
    X_train = np.concatenate(X_train_all, axis=0)
    y_train = np.concatenate(y_train_all, axis=0)
    X_test = np.concatenate(X_test_all, axis=0)
    y_test = np.concatenate(y_test_all, axis=0)
else:
    raise ValueError("No sequences generated. Check data length and sequence parameters.")

print(f"\nProcessing Complete.")
print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")
print(f"Input Feature Dimension: {X_train.shape[2]}")

"""# Task
Read the content of "/content/cleaned_a_sen_transformer.py" to identify and extract the specific 4-layer vanilla transformer class definition. Then, based on this architecture, redefine the `TimeSeriesTransformer` class to accept the 771-dimensional input (frozen embeddings + price features) and creating a new `FinBERT_TimeSeries_Model` class that integrates the FinBERT module for co-finetuning.
"""

import torch
import torch.nn as nn
import math
import torch.compiler._cache
# from transformers import BertModel #uncomment

# --- PATCH START ---
# Fix for "Artifact of type=pgo already registered" error in Colab/Torch
def safe_register(cls, artifact_cls):
    artifact_type_key = artifact_cls.type()
    if artifact_type_key in cls._artifact_types:
        return artifact_cls
    # Original logic without the assertion failure
    cls._artifact_types[artifact_type_key] = artifact_cls
    return artifact_cls

torch.compiler._cache.CacheArtifactFactory.register = classmethod(safe_register)
# --- PATCH END ---

# 1. Positional Encoding (Helper)
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x shape: (batch, seq_len, d_model)
        return x + self.pe[:, :x.size(1), :]

# 2. Time Series Transformer (Updated for Frozen Embeddings)
class TimeSeriesTransformer(nn.Module):
    def __init__(self, input_dim, d_model, num_heads, num_layers, output_dim, dropout=0.2, dim_feedforward=128):
        super(TimeSeriesTransformer, self).__init__()
        self.d_model = d_model

        # Input Projection
        self.input_projection = nn.Linear(input_dim, d_model)
        self.activation = nn.ReLU()

        # Positional Encoding
        self.pos_encoder = PositionalEncoding(d_model)
        self.pos_dropout = nn.Dropout(dropout)

        # Transformer Encoder
        # Pre-LN configuration
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=num_heads,
            batch_first=True,
            dropout=dropout,
            dim_feedforward=dim_feedforward,
            norm_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # Decoder / Output Layer
        self.decoder = nn.Linear(d_model, output_dim)

    def forward(self, x):
        # x shape: (batch, seq_len, input_dim)

        # Project and Scale
        x = self.input_projection(x) * math.sqrt(self.d_model)
        x = self.activation(x)

        # Add Positional Encoding
        x = self.pos_encoder(x)
        x = self.pos_dropout(x)

        # Transformer Encoder
        x = self.transformer_encoder(x)

        # Select last time step
        x = x[:, -1, :]

        # Output
        x = self.decoder(x)
        return x

# 3. Co-Finetuning Model Architecture
class FinBERT_TimeSeries_Model(nn.Module):
    def __init__(self, price_dim, d_model, num_heads, num_layers, output_dim, dropout=0.2, dim_feedforward=128, freeze_finbert=True):
        super(FinBERT_TimeSeries_Model, self).__init__()

        # Load Pre-trained FinBERT
        self.finbert = BertModel.from_pretrained("ProsusAI/finbert")

        # Toggle Freezing
        if freeze_finbert:
            for param in self.finbert.parameters():
                param.requires_grad = False

        # FinBERT embedding size is 768
        self.embedding_dim = 768
        combined_input_dim = price_dim + self.embedding_dim

        # Initialize the TimeSeriesTransformer component
        self.transformer = TimeSeriesTransformer(
            input_dim=combined_input_dim,
            d_model=d_model,
            num_heads=num_heads,
            num_layers=num_layers,
            output_dim=output_dim,
            dropout=dropout,
            dim_feedforward=dim_feedforward
        )

    def forward(self, price_seq, input_ids, attention_mask):
        # price_seq: (batch, seq_len, price_dim)
        # input_ids: (batch, seq_len, token_len)
        # attention_mask: (batch, seq_len, token_len)

        batch_size, seq_len, _ = price_seq.shape

        # Flatten for FinBERT processing: (batch * seq_len, token_len)
        input_ids_flat = input_ids.view(-1, input_ids.size(-1))
        attention_mask_flat = attention_mask.view(-1, attention_mask.size(-1))

        # Get FinBERT Embeddings
        # We use the CLS token (index 0) from the last hidden state
        outputs = self.finbert(input_ids=input_ids_flat, attention_mask=attention_mask_flat)
        cls_embeddings = outputs.last_hidden_state[:, 0, :] # Shape: (batch * seq_len, 768)

        # Reshape back to sequence format: (batch, seq_len, 768)
        text_embeddings = cls_embeddings.view(batch_size, seq_len, self.embedding_dim)

        # Concatenate Price Features and Text Embeddings
        # Result Shape: (batch, seq_len, price_dim + 768)
        combined_features = torch.cat((price_seq, text_embeddings), dim=2)

        # Pass through Transformer
        output = self.transformer(combined_features)

        return output

print("Model classes 'TimeSeriesTransformer' and 'FinBERT_TimeSeries_Model' defined successfully.")

"""## Train and Evaluate (Frozen)

### Subtask:
Train the `TimeSeriesTransformer` using the pre-computed frozen embeddings and evaluate its performance.

**Reasoning**:
Implement the training and evaluation pipeline for the TimeSeriesTransformer model using the pre-computed embeddings. This includes data conversion to tensors, creating DataLoaders, model instantiation, the training loop, metric calculation, and visualization of results.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import sys

# --- Fix for 'partially initialized module torch._dynamo' error ---
# Clear any corrupted dynamo modules from sys.modules to force clean re-import
modules_to_remove = [m for m in sys.modules if m.startswith("torch._dynamo")]
for m in modules_to_remove:
    del sys.modules[m]
# ------------------------------------------------------------------

# 1. Convert to PyTorch Tensors
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Ensure data is float32
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Shape (N, 1)

X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)    # Shape (N, 1)

# 2. Create DataLoaders
BATCH_SIZE = 64
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

# 3. Instantiate Model
input_dim = X_train.shape[2]  # Should be 771
d_model = 32
num_heads = 4
num_layers = 4
output_dim = 1

# Ensure TimeSeriesTransformer is defined (from previous cell)
if 'TimeSeriesTransformer' not in globals():
    raise NameError("TimeSeriesTransformer class is not defined. Please run the previous cell.")

model = TimeSeriesTransformer(
    input_dim=input_dim,
    d_model=d_model,
    num_heads=num_heads,
    num_layers=num_layers,
    output_dim=output_dim,
    dropout=0.2
).to(device)

# 4. Loss and Optimizer
criterion = nn.MSELoss()
optimizer = optim.AdamW(model.parameters(), lr=1e-3)

# 5. Training Loop
num_epochs = 100
train_losses = []
test_losses = []

# 4. Verify Parameters
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"\nModel Configuration:")
print(f"Total Parameters: {total_params:,}")
print(f"Trainable Parameters: {trainable_params:,}")
print(f"Percentage Trainable: {100 * trainable_params / total_params:.2f}%")

print(f"Starting training for {num_epochs} epochs...")

for epoch in range(num_epochs):
    model.train()
    running_train_loss = 0.0

    for X_batch, y_batch in train_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)

        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()

        running_train_loss += loss.item() * X_batch.size(0)

    epoch_train_loss = running_train_loss / len(train_loader.dataset)
    train_losses.append(epoch_train_loss)

    # Evaluation Phase
    model.eval()
    running_test_loss = 0.0
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            running_test_loss += loss.item() * X_batch.size(0)

    epoch_test_loss = running_test_loss / len(test_loader.dataset)
    test_losses.append(epoch_test_loss)

    print(f"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {epoch_train_loss:.6f} | Test Loss: {epoch_test_loss:.6f}")

# 6. Evaluation Metrics
model.eval()
with torch.no_grad():
    test_predictions = []
    test_targets = []
    for X_batch, y_batch in test_loader:
        X_batch = X_batch.to(device)
        preds = model(X_batch).cpu().numpy()
        test_predictions.extend(preds)
        test_targets.extend(y_batch.numpy())

test_predictions = np.array(test_predictions).flatten()
test_targets = np.array(test_targets).flatten()

mse = mean_squared_error(test_targets, test_predictions)
mae = mean_absolute_error(test_targets, test_predictions)
r2 = r2_score(test_targets, test_predictions)

print("\nFinal Evaluation Metrics:")
print(f"Mean Squared Error (MSE): {mse:.6f}")
print(f"Mean Absolute Error (MAE): {mae:.6f}")
print(f"R2 Score: {r2:.6f}")

# 7. Plot Results (Subset for clarity)
plt.figure(figsize=(12, 6))
plt.plot(test_targets[:100], label='Actual', alpha=0.7)
plt.plot(test_predictions[:100], label='Predicted', alpha=0.7)
plt.title('Actual vs Predicted (First 100 Test Samples)')
plt.xlabel('Sample Index')
plt.ylabel('Scaled Price')
plt.legend()
plt.show()

"""# Task
Create a custom dataset class `TextStockDataset` and data loaders for the co-finetuning model. First, aggregate the `lsa_summary` in `df_news` by stock and date (joining multiple summaries with a space). Merge this text data with `df_stocks` and impute missing text values using forward fill to maintain sentiment persistence. Then, scale the numerical features (Open, Close, Volume) using a `MinMaxScaler` fitted only on the training data (first 80% per stock). Tokenize the aggregated text using the FinBERT tokenizer with a reduced `max_length` (e.g., 32) to manage memory constraints given the sequence length. Finally, generate `train_loader` and `test_loader` returning sequences of price features, token IDs, attention masks, and targets.

# Task
Create a `TextStockDataset` class and generate DataLoaders for the co-finetuning phase.

First, aggregate the news summaries in `df_news` by `stock_symbol` and `date`, joining multiple summaries with a space. Merge this aggregated text data with `df_stocks` ensuring dates are normalized. Impute missing text values using forward fill to maintain sentiment persistence, filling any remaining gaps with empty strings.

Next, iterate through each stock to process the data:
1.  Split the data into training (80%) and testing (20%) sets chronologically.
2.  Fit a `MinMaxScaler` on the training numerical features (`open`, `close`, `volume`) and transform both sets.
3.  Tokenize the text column using the `ProsusAI/finbert` tokenizer with `max_length=32`, `padding='max_length'`, and `truncation=True`.
4.  Generate sliding window sequences of length 50 for:
    *   Scaled price features.
    *   Token IDs.
    *   Attention masks.
    *   Target variables (Close price 3 days into the future).

Finally, define the `TextStockDataset` class to handle these sequences and instantiate `train_loader` and `test_loader` with a batch size of 32.

## Prepare Data and Generate Sequences

### Subtask:
Aggregate news, merge with stocks, and generate aligned sequences of prices and tokenized text for co-finetuning.

**Reasoning**:
Execute the data preparation pipeline for co-finetuning. This involves aggregating news, merging with stock data, imputing text, scaling features per stock, tokenizing, and generating aligned sequences for training and testing.
"""

import numpy as np
import pandas as pd
from transformers import BertTokenizer
from sklearn.preprocessing import MinMaxScaler

# 1. Initialize Tokenizer
print("Initializing FinBERT tokenizer...")
tokenizer = BertTokenizer.from_pretrained("ProsusAI/finbert")

# 2. Prepare Dataframes
# Ensure dates are normalized datetimes and column names align
df_news_clean = df_news.copy()
if 'timestamp' in df_news_clean.columns:
    df_news_clean.rename(columns={'timestamp': 'date'}, inplace=True)
df_news_clean['date'] = pd.to_datetime(df_news_clean['date']).dt.normalize()

df_stocks_clean = df_stocks.copy()
df_stocks_clean['date'] = pd.to_datetime(df_stocks_clean['date']).dt.normalize()

# Aggregate News: Group by stock/date and join summaries
print("Aggregating news...")
news_agg = df_news_clean.groupby(['stock_symbol', 'date'])['lsa_summary'].apply(lambda x: ' '.join(x.dropna().astype(str))).reset_index()
news_agg.rename(columns={'lsa_summary': 'summary'}, inplace=True)

# Merge with Stocks
print("Merging stock and news data...")
merged_df = pd.merge(df_stocks_clean, news_agg, on=['stock_symbol', 'date'], how='left')

# 3. Impute Text
# Forward fill text to persist sentiment, then fill remaining with empty string
merged_df['summary'] = merged_df.groupby('stock_symbol')['summary'].ffill()
merged_df['summary'] = merged_df['summary'].fillna("")

# 4. Process per Stock (Split, Scale, Tokenize, Generate Sequences)
train_prices_list, train_ids_list, train_masks_list, train_targets_list = [], [], [], []
test_prices_list, test_ids_list, test_masks_list, test_targets_list = [], [], [], []

SEQ_LEN = 50
HORIZON = 3
PRICE_COLS = ['open', 'close', 'volume']

def create_multimodal_sequences(prices, input_ids, masks, target_col_idx, seq_len, horizon):
    X_p, X_id, X_m, y = [], [], [], []
    # Ensure we have enough data points
    if len(prices) < seq_len + horizon:
        return np.array([]), np.array([]), np.array([]), np.array([])

    for i in range(len(prices) - seq_len - horizon + 1):
        # Inputs: i to i + seq_len
        X_p.append(prices[i : i + seq_len])
        X_id.append(input_ids[i : i + seq_len])
        X_m.append(masks[i : i + seq_len])
        # Target: i + seq_len + horizon - 1
        y.append(prices[i + seq_len + horizon - 1, target_col_idx])

    return np.array(X_p), np.array(X_id), np.array(X_m), np.array(y)

print("Processing stocks (Split -> Scale -> Tokenize -> Sequence)...")
for symbol, group in merged_df.groupby('stock_symbol'):
    group = group.sort_values('date')

    # Split (80/20)
    n = len(group)
    split_idx = int(n * 0.8)

    train_group = group.iloc[:split_idx]
    test_group = group.iloc[split_idx:]

    if len(train_group) <= SEQ_LEN + HORIZON or len(test_group) <= SEQ_LEN + HORIZON:
        continue

    # Scale Numerical Features
    scaler = MinMaxScaler()
    train_prices_scaled = scaler.fit_transform(train_group[PRICE_COLS].values)
    test_prices_scaled = scaler.transform(test_group[PRICE_COLS].values)

    # Tokenize Text
    # Using return_tensors='np' for direct numpy array creation
    train_encodings = tokenizer(
        train_group['summary'].tolist(),
        max_length=32,
        padding='max_length',
        truncation=True,
        return_tensors='np'
    )
    test_encodings = tokenizer(
        test_group['summary'].tolist(),
        max_length=32,
        padding='max_length',
        truncation=True,
        return_tensors='np'
    )

    # Generate Sequences
    # Target 'close' is at index 1
    target_idx = 1

    tr_p, tr_id, tr_m, tr_y = create_multimodal_sequences(
        train_prices_scaled, train_encodings['input_ids'], train_encodings['attention_mask'],
        target_idx, SEQ_LEN, HORIZON
    )

    te_p, te_id, te_m, te_y = create_multimodal_sequences(
        test_prices_scaled, test_encodings['input_ids'], test_encodings['attention_mask'],
        target_idx, SEQ_LEN, HORIZON
    )

    if len(tr_p) > 0:
        train_prices_list.append(tr_p)
        train_ids_list.append(tr_id)
        train_masks_list.append(tr_m)
        train_targets_list.append(tr_y)

    if len(te_p) > 0:
        test_prices_list.append(te_p)
        test_ids_list.append(te_id)
        test_masks_list.append(te_m)
        test_targets_list.append(te_y)

# 6. Concatenate
if train_prices_list:
    train_prices = np.concatenate(train_prices_list, axis=0)
    train_input_ids = np.concatenate(train_ids_list, axis=0)
    train_masks = np.concatenate(train_masks_list, axis=0)
    train_targets = np.concatenate(train_targets_list, axis=0)

    test_prices = np.concatenate(test_prices_list, axis=0)
    test_input_ids = np.concatenate(test_ids_list, axis=0)
    test_masks = np.concatenate(test_masks_list, axis=0)
    test_targets = np.concatenate(test_targets_list, axis=0)

    # 7. Verify Shapes
    print("\nData Preparation Complete.")
    print(f"Train Prices: {train_prices.shape}")
    print(f"Train Input IDs: {train_input_ids.shape}")
    print(f"Train Masks: {train_masks.shape}")
    print(f"Train Targets: {train_targets.shape}")
    print("-" * 30)
    print(f"Test Prices: {test_prices.shape}")
    print(f"Test Input IDs: {test_input_ids.shape}")
    print(f"Test Masks: {test_masks.shape}")
    print(f"Test Targets: {test_targets.shape}")
else:
    print("No sequences generated. Check data lengths.")

"""## Create Custom Dataset and DataLoaders

### Subtask:
Define a PyTorch Dataset class to handle the multimodal sequences and create DataLoaders.

**Reasoning**:
Define the TextStockDataset class and instantiate DataLoaders for training and testing, then verify the batch shapes.
"""

from torch.utils.data import Dataset, DataLoader
import torch

# 1. Define Dataset Class
class TextStockDataset(Dataset):
    def __init__(self, prices, input_ids, masks, targets):
        self.prices = prices
        self.input_ids = input_ids
        self.masks = masks
        self.targets = targets

    def __len__(self):
        return len(self.prices)

    def __getitem__(self, idx):
        # Convert to tensors
        price = torch.tensor(self.prices[idx], dtype=torch.float)
        input_id = torch.tensor(self.input_ids[idx], dtype=torch.long)
        mask = torch.tensor(self.masks[idx], dtype=torch.long)
        target = torch.tensor(self.targets[idx], dtype=torch.float)

        return price, input_id, mask, target

# 2. Instantiate Datasets
train_dataset = TextStockDataset(train_prices, train_input_ids, train_masks, train_targets)
test_dataset = TextStockDataset(test_prices, test_input_ids, test_masks, test_targets)

# 3. Create DataLoaders
BATCH_SIZE = 32
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

print(f"Train Loader batches: {len(train_loader)}")
print(f"Test Loader batches: {len(test_loader)}")

# 4. Verify Batch Shapes
# Fetch one batch
example_prices, example_input_ids, example_masks, example_targets = next(iter(train_loader))

print("\n--- Batch Shape Verification ---")
print(f"Price Batch Shape: {example_prices.shape}")       # Expected: (32, 50, 3)
print(f"Input IDs Batch Shape: {example_input_ids.shape}") # Expected: (32, 50, 32)
print(f"Mask Batch Shape: {example_masks.shape}")          # Expected: (32, 50, 32)
print(f"Target Batch Shape: {example_targets.shape}")      # Expected: (32,)

"""## Initialize Model with Partial Unfreezing

### Subtask:
Instantiate the FinBERT_TimeSeries_Model and configure parameter freezing to fine-tune only the last two layers of FinBERT.

"""

import torch
import torch.nn as nn
import math
import warnings
from transformers import BertModel

# Filter specific warning from PyTorch Transformer
warnings.filterwarnings("ignore", message="enable_nested_tensor is True, but self.use_nested_tensor is False")

# --- Define Model Classes (Redefining to ensure availability) ---

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:, :x.size(1), :]

class TimeSeriesTransformer(nn.Module):
    def __init__(self, input_dim, d_model, num_heads, num_layers, output_dim, dropout=0.2, dim_feedforward=128):
        super(TimeSeriesTransformer, self).__init__()
        self.d_model = d_model
        self.input_projection = nn.Linear(input_dim, d_model)
        self.activation = nn.ReLU()
        self.pos_encoder = PositionalEncoding(d_model)
        self.pos_dropout = nn.Dropout(dropout)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=num_heads,
            batch_first=True,
            dropout=dropout,
            dim_feedforward=dim_feedforward,
            norm_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.decoder = nn.Linear(d_model, output_dim)

    def forward(self, x):
        x = self.input_projection(x) * math.sqrt(self.d_model)
        x = self.activation(x)
        x = self.pos_encoder(x)
        x = self.pos_dropout(x)
        x = self.transformer_encoder(x)
        x = x[:, -1, :]
        x = self.decoder(x)
        return x

class FinBERT_TimeSeries_Model(nn.Module):
    def __init__(self, price_dim, d_model, num_heads, num_layers, output_dim, dropout=0.2, dim_feedforward=128, freeze_finbert=True):
        super(FinBERT_TimeSeries_Model, self).__init__()
        self.finbert = BertModel.from_pretrained("ProsusAI/finbert")
        if freeze_finbert:
            for param in self.finbert.parameters():
                param.requires_grad = False
        self.embedding_dim = 768
        combined_input_dim = price_dim + self.embedding_dim
        self.transformer = TimeSeriesTransformer(
            input_dim=combined_input_dim,
            d_model=d_model,
            num_heads=num_heads,
            num_layers=num_layers,
            output_dim=output_dim,
            dropout=dropout,
            dim_feedforward=dim_feedforward
        )

    def forward(self, price_seq, input_ids, attention_mask):
        batch_size, seq_len, _ = price_seq.shape
        input_ids_flat = input_ids.view(-1, input_ids.size(-1))
        attention_mask_flat = attention_mask.view(-1, attention_mask.size(-1))
        outputs = self.finbert(input_ids=input_ids_flat, attention_mask=attention_mask_flat)
        cls_embeddings = outputs.last_hidden_state[:, 0, :]
        text_embeddings = cls_embeddings.view(batch_size, seq_len, self.embedding_dim)
        combined_features = torch.cat((price_seq, text_embeddings), dim=2)
        output = self.transformer(combined_features)
        return output

# --- Execution Logic ---

# 1. Define Hyperparameters
price_dim = 3
d_model = 32
num_heads = 4
num_layers = 4
output_dim = 1
dropout = 0.2

# Check device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# 2. Instantiate Model
print("Instantiating FinBERT_TimeSeries_Model...")
model = FinBERT_TimeSeries_Model(
    price_dim=price_dim,
    d_model=d_model,
    num_heads=num_heads,
    num_layers=num_layers,
    output_dim=output_dim,
    dropout=dropout,
    freeze_finbert=True
).to(device)

# 3. Selectively Unfreeze
print("Unfreezing last 1 layers of FinBERT...")
for name, param in model.finbert.named_parameters():
    # if "encoder.layer.10" in name or "encoder.layer.11" in name or "pooler" in name:
    if "encoder.layer.11" in name or "pooler" in name:
        param.requires_grad = True

# 4. Verify Parameters
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"\nModel Configuration:")
print(f"Total Parameters: {total_params:,}")
print(f"Trainable Parameters: {trainable_params:,}")
print(f"Percentage Trainable: {100 * trainable_params / total_params:.2f}%")

"""# Task
Configure an `AdamW` optimizer with differential learning rates, assigning `1e-5` to the unfrozen FinBERT parameters and `1e-3` to the `TimeSeriesTransformer` and other components. Train the `FinBERT_TimeSeries_Model` for 5 epochs using the `train_loader` and `test_loader`. Calculate and print the Mean Squared Error (MSE), Mean Absolute Error (MAE), and R-squared (RÂ²) metrics for the test set, and plot the training and validation loss curves to visualize convergence.

## Train with Differential Learning Rates

### Subtask:
Configure the optimizer with differential learning rates, train the co-finetuning model for 5 epochs, and evaluate its performance.

**Reasoning**:
Implement the training loop with differential learning rates as specified. I will separate the parameters, initialize the optimizer, run the loop for 5 epochs, calculate metrics, and plot the loss curves.
"""

import torch.optim as optim
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt

# 1. Separate Parameters for Differential Learning Rates
finbert_params = [p for n, p in model.named_parameters() if 'finbert' in n and p.requires_grad]
other_params = [p for n, p in model.named_parameters() if 'finbert' not in n and p.requires_grad]

print(f"FinBERT parameters to optimize: {len(finbert_params)}")
print(f"Other parameters to optimize: {len(other_params)}")

# 2. Initialize Optimizer
optimizer = optim.AdamW([
    {'params': finbert_params, 'lr': 1e-5},
    {'params': other_params, 'lr': 1e-3}
])

criterion = nn.MSELoss()

# 3. Training Loop
num_epochs = 3
train_losses = []
val_losses = []

# Containers for final evaluation
final_preds = []
final_targets = []

print(f"\nStarting co-finetuning for {num_epochs} epochs...")

for epoch in range(num_epochs):
    print(f"Epoch {epoch + 1}/{num_epochs}")
    # --- Training Phase ---
    model.train()
    running_train_loss = 0.0
    train_loader_length = len(train_loader)

    for k, (prices, input_ids, masks, targets) in enumerate(train_loader):
        prices = prices.to(device)
        input_ids = input_ids.to(device)
        masks = masks.to(device)
        targets = targets.to(device)

        optimizer.zero_grad()
        print(f"starting batch forward pass for iteration {k} of {train_loader_length}")
        outputs = model(prices, input_ids, masks)
        # outputs: (batch, 1), targets: (batch)
        loss = criterion(outputs.squeeze(), targets)
        loss.backward()
        optimizer.step()

        running_train_loss += loss.item() * prices.size(0)

    epoch_train_loss = running_train_loss / len(train_loader.dataset)
    train_losses.append(epoch_train_loss)

    # --- Validation Phase ---
    model.eval()
    running_val_loss = 0.0
    epoch_preds = []
    epoch_targets = []

    with torch.no_grad():
        for prices, input_ids, masks, targets in test_loader:
            prices = prices.to(device)
            input_ids = input_ids.to(device)
            masks = masks.to(device)
            targets = targets.to(device)

            outputs = model(prices, input_ids, masks)
            loss = criterion(outputs.squeeze(), targets)

            running_val_loss += loss.item() * prices.size(0)

            epoch_preds.extend(outputs.squeeze().cpu().numpy())
            epoch_targets.extend(targets.cpu().numpy())

    epoch_val_loss = running_val_loss / len(test_loader.dataset)
    val_losses.append(epoch_val_loss)

    print(f"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {epoch_train_loss:.6f} | Val Loss: {epoch_val_loss:.6f}")

    # Save predictions from the last epoch
    if epoch == num_epochs - 1:
        final_preds = epoch_preds
        final_targets = epoch_targets

# 4. Calculate Metrics
mse = mean_squared_error(final_targets, final_preds)
mae = mean_absolute_error(final_targets, final_preds)
r2 = r2_score(final_targets, final_preds)

print("\nFinal Co-Finetuning Performance:")
print(f"Mean Squared Error (MSE): {mse:.6f}")
print(f"Mean Absolute Error (MAE): {mae:.6f}")
print(f"R2 Score: {r2:.6f}")

# 5. Plot Loss Curves
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Train Loss', marker='o')
plt.plot(val_losses, label='Validation Loss', marker='x')
plt.title('Co-Finetuning Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('MSE Loss')
plt.legend()
plt.grid(True)
plt.show()

