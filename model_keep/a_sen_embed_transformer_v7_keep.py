# -*- coding: utf-8 -*-
"""A_sen_embed_transformer_v7_keep.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15nlGexciyY-Wiu4lpH9LAaxSlklEDul4

I have generated a plan to modify the notebook to use FinBERT embeddings instead of scalar sentiment scores. This involves extracting the embeddings, updating the data pipeline to handle high-dimensional vectors, and modifying the model architecture. I will also include a setup for co-finetuning FinBERT, though I'll add checks for memory constraints given the sequence length.

Install Dependencies: Install the transformers library to load the FinBERT model and tokenizer.
Extract FinBERT Embeddings: Load the pre-trained FinBERT model. Process the df_news dataframe to generate 768-dimensional embeddings for each article. Aggregate these embeddings by stock and date (e.g., using mean pooling) to create a daily sentiment vector.
Update Data Processing: Modify the merging logic to join the 768-dimensional sentiment vectors with the stock price data. Update the impute_decay function to handle vector imputation instead of scalar values. Scale the price features while keeping embeddings (or normalizing them separately) and create time-series sequences.
Define Transformer Model (Frozen Embeddings): Update the TimeSeriesTransformer class to accept the new input dimension (3 price features + 768 embedding features). This model will use the pre-computed frozen embeddings.
Define Co-Finetuning Model Architecture: Create a new model class FinBERT_TimeSeries_Model that integrates the FinBERT module directly. This class will accept tokenized text inputs and price sequences, allowing for end-to-end fine-tuning of FinBERT weights. (Note: This will be structured to allow toggling between frozen and unfrozen states).
Train and Evaluate: Set up the training loop to train the modified transformer using the frozen embeddings. Evaluate the performance on the test set and visualize the predictions.
Final Task: Summarize the changes made to the architecture and the results of using embeddings versus the original scalar approach.

# Setup

## Install
"""

!pip install awswrangler

"""## Import"""

import pandas as pd
import awswrangler as wr
import boto3
import numpy as np
import matplotlib.pyplot as plt
from getpass import getpass
import os

import torch
import torch.nn as nn
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import DataLoader, Dataset

"""## Constants"""

# --- IMPORTANT: Set these variables before running ---
AWS_REGION = 'us-east-2'
S3_STAGING_DIR = 's3://cs230-market-data-2025/athena-query-results/'
ATHENA_DB = 'cs230_finance_data'
# Querying more data for a small training run
# SQL_QUERY = "SELECT concatarticles1, concatarticles2, vol_1_vs_2 FROM paired_vixy_w_titles_v3 WHERE vol_1_vs_2 is not null ORDER BY RAND() LIMIT 10000"
# SQL_QUERY = "SELECT concatarticles1, concatarticles2, open_lead_close_lag_change_pct_bucket1 as bucket1, open_lead_close_lag_change_pct_bucket2 as bucket2, samebucket_open_lead_close_lag_change_pct as samebucket FROM paired_vixy_w_titles_dedupped ORDER BY RAND() LIMIT 10000"
SQL_QUERY_NEWS = "SELECT * FROM fnspid_nasdaq_news_top5_summaries"
SQL_QUERY_STOCKS = "SELECT * FROM fnspid_stock_prices_top5"

SEQ_LEN = 50 # Small for this dummy data, use 30 or 60 for real usage
FUTURE_LEN = 3

# ----------------------------------------------------

"""## Connect AWS"""

# --- AWS Authentication for Colab ---
# Prompt for AWS credentials
aws_access_key_id = getpass('Enter AWS Access Key ID: ')
aws_secret_access_key = getpass('Enter AWS Secret Access Key: ')

"""# Preprocessing

## Read data from aws
"""

print(f"\n--- Step 3: Configuration set for {ATHENA_DB} ---")
print(f"--- Step 4: Querying Data ---")
print(f"Querying data from {ATHENA_DB}....")

# Define df in a wider scope
df_news = None
df_stocks = None

try:
    # Create a boto3 session with the provided credentials
    session = boto3.Session(
        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key,
        region_name=AWS_REGION,
    )

    # Run the query and load results into a Pandas DataFrame
    df_news = wr.athena.read_sql_query(
        sql=SQL_QUERY_NEWS,
        database=ATHENA_DB,
        s3_output=S3_STAGING_DIR,
        boto3_session=session,
    )

    df_stocks = wr.athena.read_sql_query(
        sql=SQL_QUERY_STOCKS,
        database=ATHENA_DB,
        s3_output=S3_STAGING_DIR,
        boto3_session=session,
    )
    SQL_QUERY_STOCKS

    print("\nQuery successful! Data loaded into DataFrame.")



except Exception as e:
    print(f"\nAn error occurred:")
    print(e)

df_news

"""## Process stock price"""

df = df_stocks.copy()
df['date'] = pd.to_datetime(df['date']).dt.normalize()

# Helper function to create sequences
def create_sequences(features, target, lookback=50, horizon=3):
    X, y = [], []
    # We create sequences of length 'lookback'
    # The target is 'horizon' steps after the sequence ends
    # Sequence ends at index i + lookback - 1
    # Target index is (i + lookback - 1) + horizon
    for i in range(len(features) - lookback - horizon + 1):
        X.append(features[i : i + lookback])
        y.append(target[i + lookback + horizon - 1])
    return np.array(X), np.array(y)

print("Helper function 'create_sequences' defined.")
# The actual data generation loop has been moved to the Sentiment Fusion section
# to avoid doing the work twice.

"""# Task
Install the `transformers` library. Then, load the pre-trained FinBERT model and tokenizer. Process the `df_news` DataFrame to generate 768-dimensional embeddings for each article using the 'lsa_summary' column. Finally, aggregate these embeddings by stock and date (e.g., using mean pooling) to create a daily sentiment vector.

## Install Dependencies

### Subtask:
Install the `transformers` library to load the FinBERT model and tokenizer.

**Reasoning**:
Install the transformers library to enable loading the FinBERT model and tokenizer.
"""

!pip install transformers

"""## Extract FinBERT Embeddings

### Subtask:
Generate 768-dimensional embeddings for the news summaries using the pre-trained FinBERT model and aggregate them by stock and date.

**Reasoning**:
Implement the embedding extraction process using FinBERT. This involves loading the model, processing the news summaries in batches to extract CLS token embeddings, and aggregating them by stock and date. This directly follows the provided instructions.
"""

import torch
import pandas as pd
import numpy as np
from tqdm import tqdm
from transformers import BertTokenizer, BertModel

# # 1. Setup Device
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# print(f"Using device: {device}")

# # 2. Load Model and Tokenizer
# MODEL_NAME = "ProsusAI/finbert"
# tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)
# model = BertModel.from_pretrained(MODEL_NAME)
# model.to(device)
# model.eval()

# # 3. Data Preparation
# # Check if df_news exists in global scope
# if 'df_news' not in globals():
#     raise ValueError("df_news is not defined. Please ensure the data loading step was successful.")

# summaries = df_news['lsa_summary'].fillna("").astype(str).tolist()

# # 4. Generate Embeddings
# all_embeddings = []
# batch_size = 32

# print(f"Generating embeddings for {len(summaries)} articles...")

# for i in tqdm(range(0, len(summaries), batch_size)):
#     batch_texts = summaries[i : i + batch_size]

#     # Tokenize
#     inputs = tokenizer(
#         batch_texts,
#         return_tensors="pt",
#         padding=True,
#         truncation=True,
#         max_length=256
#     ).to(device)

#     # Inference
#     with torch.no_grad():
#         outputs = model(**inputs)
#         # CLS token is at index 0
#         cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
#         all_embeddings.append(cls_embeddings)

# # 5. Aggregate
# if all_embeddings:
#     embeddings_matrix = np.vstack(all_embeddings)

#     # Create DataFrame
#     embedding_df = pd.DataFrame(embeddings_matrix)

#     # Add metadata
#     embedding_df['stock_symbol'] = df_news['stock_symbol'].values

#     # Handle date column name variations
#     date_col = 'timestamp' if 'timestamp' in df_news.columns else 'date'
#     embedding_df['date'] = pd.to_datetime(df_news[date_col]).dt.normalize()

#     # Group by stock and date
#     print("Aggregating embeddings...")
#     sentiment_embeddings = embedding_df.groupby(['stock_symbol', 'date']).mean().reset_index()

#     print(f"Sentiment embeddings shape: {sentiment_embeddings.shape}")
#     print(sentiment_embeddings.head())

# else:
#     print("No embeddings generated.")

"""# Task
Save the generated `sentiment_embeddings` DataFrame to a CSV file named "sentiment_embeddings.csv" for persistence. Then, merge these embeddings with the `df_stocks` DataFrame based on stock symbol and date. Update the data processing pipeline to handle the 768-dimensional sentiment vectors by implementing an imputation strategy for missing days (e.g., forward fill or decay) and scaling the numerical stock price features (Open, Close, Volume) while keeping the embeddings unchanged. Finally, generate the time-series sequences and split the data into training and testing sets.

## Save and Load Embeddings

### Subtask:
Save the generated embeddings to a CSV file for persistence and implement loading logic.

**Reasoning**:
I will implement the logic to save the `sentiment_embeddings` dataframe to a CSV file if it exists in memory, or load it from the file if it doesn't, ensuring the 'date' column is properly converted to datetime objects.
"""

import os
import pandas as pd

# Check if sentiment_embeddings exists in memory
if 'sentiment_embeddings' in globals():
    print("Saving embeddings to CSV...")
    sentiment_embeddings.to_csv('sentiment_embeddings.csv', index=False)
    print("Embeddings saved to 'sentiment_embeddings.csv'.")
else:
    print("Variable 'sentiment_embeddings' not found in memory. Checking for file...")
    if os.path.exists('sentiment_embeddings.csv'):
        print("Loading embeddings from 'sentiment_embeddings.csv'...")
        sentiment_embeddings = pd.read_csv('sentiment_embeddings.csv')
        sentiment_embeddings['date'] = pd.to_datetime(sentiment_embeddings['date'])
        print("Embeddings loaded successfully.")
    else:
        print("Error: 'sentiment_embeddings.csv' not found. Please run the embedding generation step first.")

# Verify
if 'sentiment_embeddings' in globals():
    print(f"Sentiment Embeddings Shape: {sentiment_embeddings.shape}")
    print(sentiment_embeddings.head())

"""**Reasoning**:
I will merge the stock data with the generated sentiment embeddings, using a forward fill strategy to impute missing sentiment values as permitted by the instructions. Then, I will scale the numerical stock features and generate time-series sequences for training and testing.


"""

import numpy as np
from sklearn.preprocessing import MinMaxScaler

# 1. Prepare Embeddings
# Rename columns to ensure they are distinct strings and avoid conflict
embedding_cols = [col for col in sentiment_embeddings.columns if col not in ['stock_symbol', 'date']]
new_col_names = {col: f"emb_{col}" for col in embedding_cols}
sentiment_embeddings_renamed = sentiment_embeddings.rename(columns=new_col_names)
embedding_feature_names = list(new_col_names.values())

# 2. Merge with Stock Data
# Ensure 'date' column in df is datetime and normalized
df['date'] = pd.to_datetime(df['date']).dt.normalize()

# Merge on stock_symbol and date (Left Join to keep all stock data)
merged_df = pd.merge(df, sentiment_embeddings_renamed, on=['stock_symbol', 'date'], how='left')

# 3. Impute Missing Embeddings
print("Imputing missing embeddings (Forward Fill then Zero Fill)...")
# Forward fill embeddings per stock to persist sentiment
merged_df[embedding_feature_names] = merged_df.groupby('stock_symbol')[embedding_feature_names].ffill()
# Fill remaining NaNs (e.g., beginning of history) with 0.0
merged_df[embedding_feature_names] = merged_df[embedding_feature_names].fillna(0.0)

# 4. Scale Price Features
print("Scaling price features...")
price_features = ['open', 'close', 'volume']
scaler = MinMaxScaler()
merged_df[price_features] = scaler.fit_transform(merged_df[price_features])

# 5. Generate Sequences
print("Generating sequences...")
X_all, y_all = [], []
stocks = merged_df['stock_symbol'].unique()

# Features: Price features + 768 embedding features
all_features = price_features + embedding_feature_names

for stock in stocks:
    stock_data = merged_df[merged_df['stock_symbol'] == stock].sort_values('date')

    # Check if enough data points exist
    if len(stock_data) > (SEQ_LEN + FUTURE_LEN):
        features = stock_data[all_features].values
        target = stock_data['close'].values # Predict Scaled Close Price

        X_stock, y_stock = create_sequences(features, target, lookback=SEQ_LEN, horizon=FUTURE_LEN)
        X_all.append(X_stock)
        y_all.append(y_stock)

# Concatenate all stocks
if X_all:
    X = np.concatenate(X_all, axis=0)
    y = np.concatenate(y_all, axis=0)
else:
    raise ValueError("No sequences generated. Check data length and sequence parameters.")

# 6. Split Train/Test
split_idx = int(len(X) * 0.8)
X_train, X_test = X[:split_idx], X[split_idx:]
y_train, y_test = y[:split_idx], y[split_idx:]

print(f"\nProcessing Complete.")
print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")
print(f"Input Feature Dimension: {X_train.shape[2]}")

"""# Task
Read the content of "/content/cleaned_a_sen_transformer.py" to identify and extract the specific 4-layer vanilla transformer class definition. Then, based on this architecture, redefine the `TimeSeriesTransformer` class to accept the 771-dimensional input (frozen embeddings + price features) and creating a new `FinBERT_TimeSeries_Model` class that integrates the FinBERT module for co-finetuning.

## Read Reference Architecture

### Subtask:
Read the content of the provided python file to extract the model architecture.

**Reasoning**:
Read the content of the provided python file to inspect the model architecture.
"""

with open('/content/cleaned_a_sen_transformer.py', 'r') as f:
    content = f.read()
    print(content)

import torch
import torch.nn as nn
import math
import torch.compiler._cache
# from transformers import BertModel #uncomment

# --- PATCH START ---
# Fix for "Artifact of type=pgo already registered" error in Colab/Torch
def safe_register(cls, artifact_cls):
    artifact_type_key = artifact_cls.type()
    if artifact_type_key in cls._artifact_types:
        return artifact_cls
    # Original logic without the assertion failure
    cls._artifact_types[artifact_type_key] = artifact_cls
    return artifact_cls

torch.compiler._cache.CacheArtifactFactory.register = classmethod(safe_register)
# --- PATCH END ---

# 1. Positional Encoding (Helper)
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x shape: (batch, seq_len, d_model)
        return x + self.pe[:, :x.size(1), :]

# 2. Time Series Transformer (Updated for Frozen Embeddings)
class TimeSeriesTransformer(nn.Module):
    def __init__(self, input_dim, d_model, num_heads, num_layers, output_dim, dropout=0.2, dim_feedforward=128):
        super(TimeSeriesTransformer, self).__init__()
        self.d_model = d_model

        # Input Projection
        self.input_projection = nn.Linear(input_dim, d_model)
        self.activation = nn.ReLU()

        # Positional Encoding
        self.pos_encoder = PositionalEncoding(d_model)
        self.pos_dropout = nn.Dropout(dropout)

        # Transformer Encoder
        # Pre-LN configuration
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=num_heads,
            batch_first=True,
            dropout=dropout,
            dim_feedforward=dim_feedforward,
            norm_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # Decoder / Output Layer
        self.decoder = nn.Linear(d_model, output_dim)

    def forward(self, x):
        # x shape: (batch, seq_len, input_dim)

        # Project and Scale
        x = self.input_projection(x) * math.sqrt(self.d_model)
        x = self.activation(x)

        # Add Positional Encoding
        x = self.pos_encoder(x)
        x = self.pos_dropout(x)

        # Transformer Encoder
        x = self.transformer_encoder(x)

        # Select last time step
        x = x[:, -1, :]

        # Output
        x = self.decoder(x)
        return x

# 3. Co-Finetuning Model Architecture
class FinBERT_TimeSeries_Model(nn.Module):
    def __init__(self, price_dim, d_model, num_heads, num_layers, output_dim, dropout=0.2, dim_feedforward=128, freeze_finbert=True):
        super(FinBERT_TimeSeries_Model, self).__init__()

        # Load Pre-trained FinBERT
        self.finbert = BertModel.from_pretrained("ProsusAI/finbert")

        # Toggle Freezing
        if freeze_finbert:
            for param in self.finbert.parameters():
                param.requires_grad = False

        # FinBERT embedding size is 768
        self.embedding_dim = 768
        combined_input_dim = price_dim + self.embedding_dim

        # Initialize the TimeSeriesTransformer component
        self.transformer = TimeSeriesTransformer(
            input_dim=combined_input_dim,
            d_model=d_model,
            num_heads=num_heads,
            num_layers=num_layers,
            output_dim=output_dim,
            dropout=dropout,
            dim_feedforward=dim_feedforward
        )

    def forward(self, price_seq, input_ids, attention_mask):
        # price_seq: (batch, seq_len, price_dim)
        # input_ids: (batch, seq_len, token_len)
        # attention_mask: (batch, seq_len, token_len)

        batch_size, seq_len, _ = price_seq.shape

        # Flatten for FinBERT processing: (batch * seq_len, token_len)
        input_ids_flat = input_ids.view(-1, input_ids.size(-1))
        attention_mask_flat = attention_mask.view(-1, attention_mask.size(-1))

        # Get FinBERT Embeddings
        # We use the CLS token (index 0) from the last hidden state
        outputs = self.finbert(input_ids=input_ids_flat, attention_mask=attention_mask_flat)
        cls_embeddings = outputs.last_hidden_state[:, 0, :] # Shape: (batch * seq_len, 768)

        # Reshape back to sequence format: (batch, seq_len, 768)
        text_embeddings = cls_embeddings.view(batch_size, seq_len, self.embedding_dim)

        # Concatenate Price Features and Text Embeddings
        # Result Shape: (batch, seq_len, price_dim + 768)
        combined_features = torch.cat((price_seq, text_embeddings), dim=2)

        # Pass through Transformer
        output = self.transformer(combined_features)

        return output

print("Model classes 'TimeSeriesTransformer' and 'FinBERT_TimeSeries_Model' defined successfully.")

"""## Train and Evaluate (Frozen)

### Subtask:
Train the `TimeSeriesTransformer` using the pre-computed frozen embeddings and evaluate its performance.

**Reasoning**:
Implement the training and evaluation pipeline for the TimeSeriesTransformer model using the pre-computed embeddings. This includes data conversion to tensors, creating DataLoaders, model instantiation, the training loop, metric calculation, and visualization of results.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import sys

# --- Fix for 'partially initialized module torch._dynamo' error ---
# Clear any corrupted dynamo modules from sys.modules to force clean re-import
modules_to_remove = [m for m in sys.modules if m.startswith("torch._dynamo")]
for m in modules_to_remove:
    del sys.modules[m]
# ------------------------------------------------------------------

# 1. Convert to PyTorch Tensors
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Ensure data is float32
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Shape (N, 1)

X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)    # Shape (N, 1)

# 2. Create DataLoaders
BATCH_SIZE = 64
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

# 3. Instantiate Model
input_dim = X_train.shape[2]  # Should be 771
d_model = 32
num_heads = 4
num_layers = 4
output_dim = 1

# Ensure TimeSeriesTransformer is defined (from previous cell)
if 'TimeSeriesTransformer' not in globals():
    raise NameError("TimeSeriesTransformer class is not defined. Please run the previous cell.")

model = TimeSeriesTransformer(
    input_dim=input_dim,
    d_model=d_model,
    num_heads=num_heads,
    num_layers=num_layers,
    output_dim=output_dim,
    dropout=0.2
).to(device)

# 4. Loss and Optimizer
criterion = nn.MSELoss()
optimizer = optim.AdamW(model.parameters(), lr=1e-3)

# 5. Training Loop
num_epochs = 15
train_losses = []
test_losses = []

print(f"Starting training for {num_epochs} epochs...")

for epoch in range(num_epochs):
    model.train()
    running_train_loss = 0.0

    for X_batch, y_batch in train_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)

        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()

        running_train_loss += loss.item() * X_batch.size(0)

    epoch_train_loss = running_train_loss / len(train_loader.dataset)
    train_losses.append(epoch_train_loss)

    # Evaluation Phase
    model.eval()
    running_test_loss = 0.0
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            running_test_loss += loss.item() * X_batch.size(0)

    epoch_test_loss = running_test_loss / len(test_loader.dataset)
    test_losses.append(epoch_test_loss)

    print(f"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {epoch_train_loss:.6f} | Test Loss: {epoch_test_loss:.6f}")

# 6. Evaluation Metrics
model.eval()
with torch.no_grad():
    test_predictions = []
    test_targets = []
    for X_batch, y_batch in test_loader:
        X_batch = X_batch.to(device)
        preds = model(X_batch).cpu().numpy()
        test_predictions.extend(preds)
        test_targets.extend(y_batch.numpy())

test_predictions = np.array(test_predictions).flatten()
test_targets = np.array(test_targets).flatten()

mse = mean_squared_error(test_targets, test_predictions)
mae = mean_absolute_error(test_targets, test_predictions)
r2 = r2_score(test_targets, test_predictions)

print("\nFinal Evaluation Metrics:")
print(f"Mean Squared Error (MSE): {mse:.6f}")
print(f"Mean Absolute Error (MAE): {mae:.6f}")
print(f"R2 Score: {r2:.6f}")

# 7. Plot Results (Subset for clarity)
plt.figure(figsize=(12, 6))
plt.plot(test_targets[:100], label='Actual', alpha=0.7)
plt.plot(test_predictions[:100], label='Predicted', alpha=0.7)
plt.title('Actual vs Predicted (First 100 Test Samples)')
plt.xlabel('Sample Index')
plt.ylabel('Scaled Price')
plt.legend()
plt.show()

"""# Task
Create a custom dataset class `TextStockDataset` and data loaders for the co-finetuning model. First, aggregate the `lsa_summary` in `df_news` by stock and date (joining multiple summaries with a space). Merge this text data with `df_stocks` and impute missing text values using forward fill to maintain sentiment persistence. Then, scale the numerical features (Open, Close, Volume) using a `MinMaxScaler` fitted only on the training data (first 80% per stock). Tokenize the aggregated text using the FinBERT tokenizer with a reduced `max_length` (e.g., 32) to manage memory constraints given the sequence length. Finally, generate `train_loader` and `test_loader` returning sequences of price features, token IDs, attention masks, and targets.

# Task
Refactor the data preparation pipeline to implement 'Lazy Loading' for memory efficiency. First, merge `df_stocks` and `sentiment_embeddings` (loading from "sentiment_embeddings.csv" if not in memory), ensuring all numerical columns are immediately downcasted to `float32` to save RAM. Perform a strict time-based split (80% train, 20% test) on the *flat* DataFrame for each stock, and apply `MinMaxScaler` to the price features (fitting only on the training set). Then, define a custom PyTorch `LazyStockDataset` class that accepts the flat feature matrix and generates sequences on-the-fly using pre-calculated valid indices, ensuring sequences do not cross stock boundaries. Finally, invoke `gc.collect()` to clean up intermediate variables.

## Optimize Data Pipeline (Lazy Loading)

### Subtask:
Refactor the data pipeline to prepare flat, memory-efficient float32 arrays for training and testing, implementing strict time-based splitting and scaling.

**Reasoning**:
Refactor the data pipeline to perform a memory-efficient, strict time-based split. This involves merging data, creating flat float32 arrays for training and testing, scaling based only on training data, and generating index tuples to track stock sequences without creating all sliding windows upfront.
"""

import gc
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler

# 1. Load Embeddings if not present
if 'sentiment_embeddings' not in globals():
    if os.path.exists('sentiment_embeddings.csv'):
        print("Loading embeddings from CSV...")
        sentiment_embeddings = pd.read_csv('sentiment_embeddings.csv')
        sentiment_embeddings['date'] = pd.to_datetime(sentiment_embeddings['date'])
    else:
        raise FileNotFoundError("sentiment_embeddings.csv not found. Please run the embedding generation step.")

# 2. Merge Data
print("Merging Stocks and Embeddings...")
# Ensure dates are normalized
df_stocks['date'] = pd.to_datetime(df_stocks['date']).dt.normalize()
sentiment_embeddings['date'] = pd.to_datetime(sentiment_embeddings['date']).dt.normalize()

# Handle column names if needed (ensure emb_ prefix)
emb_cols = [c for c in sentiment_embeddings.columns if c not in ['stock_symbol', 'date']]
rename_map = {c: f"emb_{c}" for c in emb_cols if not str(c).startswith('emb_')}
sentiment_embeddings_renamed = sentiment_embeddings.rename(columns=rename_map)

merged_df = pd.merge(df_stocks, sentiment_embeddings_renamed, on=['stock_symbol', 'date'], how='left')

# 3. Impute & Cast to Float32
print("Imputing and optimizing memory...")
embedding_feature_names = [col for col in merged_df.columns if col.startswith('emb_')]
merged_df[embedding_feature_names] = merged_df.groupby('stock_symbol')[embedding_feature_names].ffill()
merged_df[embedding_feature_names] = merged_df[embedding_feature_names].fillna(0.0)

# Cast numerical columns to float32
float_cols = ['open', 'close', 'volume'] + embedding_feature_names
merged_df[float_cols] = merged_df[float_cols].astype('float32')

# 4. Sort
merged_df = merged_df.sort_values(['stock_symbol', 'date']).reset_index(drop=True)

# 5. Split & Collect Chunks
print("Splitting data (Strict Time-Based)...")
train_chunks = []
test_chunks = []
train_group_indices = []
test_group_indices = []

current_train_idx = 0
current_test_idx = 0

unique_stocks = merged_df['stock_symbol'].unique()

for stock in unique_stocks:
    stock_df = merged_df[merged_df['stock_symbol'] == stock]
    n_rows = len(stock_df)

    # Skip if not enough data for at least one sequence
    if n_rows < (SEQ_LEN + FUTURE_LEN + 2):
        continue

    split_idx = int(n_rows * 0.8)

    # Slices
    train_slice = stock_df.iloc[:split_idx]
    test_slice = stock_df.iloc[split_idx:]

    # Track indices for Lazy Dataset (start, end)
    if len(train_slice) > 0:
        train_chunks.append(train_slice)
        train_group_indices.append((current_train_idx, current_train_idx + len(train_slice)))
        current_train_idx += len(train_slice)

    if len(test_slice) > 0:
        test_chunks.append(test_slice)
        test_group_indices.append((current_test_idx, current_test_idx + len(test_slice)))
        current_test_idx += len(test_slice)

# 6. Concatenate
print("Concatenating chunks...")
train_df = pd.concat(train_chunks, axis=0, ignore_index=True)
test_df = pd.concat(test_chunks, axis=0, ignore_index=True)

# 7. Scale (Fit on Train, Transform Both)
print("Scaling features...")
scaler = MinMaxScaler()
price_cols = ['open', 'close', 'volume']

# Fit on Train
train_df[price_cols] = scaler.fit_transform(train_df[price_cols])
# Transform Test
test_df[price_cols] = scaler.transform(test_df[price_cols])

# 8. Extract Flat Numpy Arrays (Float32)
print("Extracting flat arrays...")
feature_cols = price_cols + embedding_feature_names

X_train_flat = train_df[feature_cols].values.astype(np.float32)
y_train_flat = train_df['close'].values.astype(np.float32)

X_test_flat = test_df[feature_cols].values.astype(np.float32)
y_test_flat = test_df['close'].values.astype(np.float32)

# 9. Cleanup
del merged_df, train_df, test_df, train_chunks, test_chunks, sentiment_embeddings, sentiment_embeddings_renamed
gc.collect()

print("\nData Optimization Complete.")
print(f"X_train_flat shape: {X_train_flat.shape}")
print(f"X_test_flat shape: {X_test_flat.shape}")
print(f"Num Train Groups: {len(train_group_indices)}")
print(f"Num Test Groups: {len(test_group_indices)}")

"""# Task
Define a custom PyTorch `Dataset` class named `LazyStockDataset` designed for memory efficiency. The class should accept flat feature and target arrays along with a list of `group_indices` (start, end tuples) to delineate individual stock data. In the `__init__` method, pre-calculate a list of valid start indices to ensure that sequences of length `SEQ_LEN` with a prediction `horizon` do not cross stock boundaries. Implement `__getitem__` to lazily slice the data on-the-fly. Instantiate this dataset for both training and testing using the previously generated flat arrays (`X_train_flat`, `X_test_flat`) and group indices. Finally, create `DataLoader` instances for both sets with a batch size of 64 and shuffle enabled for training.

# Plan:
* **Define LazyStockDataset**: Define a PyTorch `Dataset` class that accepts the flat feature matrix (`X_train_flat`) and group indices (`train_group_indices`). It will pre-calculate a list of valid start indices that ensure sequences `[i : i+seq_len]` never cross stock boundaries or the split point. Create `DataLoader`s for training and testing.
* **Train Frozen Model (Memory Efficient)**: Instantiate the `TimeSeriesTransformer` (using the 4-layer architecture as requested) and train it using the `LazyStockDataset`. This verifies the baseline performance without data leakage.
* **Prepare Text Data & Co-Finetune**: Extend the lazy loading approach to text data. I will generate a flat array of tokenized `input_ids` aligned with the stock data. Then, update the `LazyStockDataset` to return `(price_seq, input_ids, attention_mask)` and train the `FinBERT_TimeSeries_Model`.
* **Final Task**: Summarize the final architecture, the memory optimization strategy, and the comparative results of the frozen vs. co-finetuned models.

## Define LazyStockDataset

### Subtask:
Define a custom memory-efficient Dataset class and create DataLoaders for the flat training and testing data.

**Reasoning**:
Define the `LazyStockDataset` class to handle efficient data loading from flat arrays using pre-calculated valid indices, and initialize the DataLoaders for training and testing.
"""

import torch
from torch.utils.data import Dataset, DataLoader

class LazyStockDataset(Dataset):
    def __init__(self, features, targets, group_indices, seq_len=50, horizon=3):
        self.features = features
        self.targets = targets
        self.seq_len = seq_len
        self.horizon = horizon
        self.valid_starts = []

        # Pre-calculate valid start indices
        # A sequence starting at i uses indices [i : i+seq_len]
        # The target is at index i + seq_len + horizon - 1
        # This target index must be < group_end
        for start, end in group_indices:
            # Last valid start index is such that target index is end - 1
            # i + seq_len + horizon - 1 = end - 1
            # i = end - seq_len - horizon
            # We need range(start, end - seq_len - horizon + 1)
            last_valid_start = end - seq_len - horizon
            if last_valid_start >= start:
                self.valid_starts.extend(range(start, last_valid_start + 1))

    def __len__(self):
        return len(self.valid_starts)

    def __getitem__(self, idx):
        global_idx = self.valid_starts[idx]

        # Slice sequence
        x = self.features[global_idx : global_idx + self.seq_len]

        # Get target
        target_idx = global_idx + self.seq_len + self.horizon - 1
        y = self.targets[target_idx]

        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(-1)

# Instantiate Datasets
# Check if variables exist (they should from previous step)
if 'X_train_flat' in globals() and 'train_group_indices' in globals():
    train_dataset = LazyStockDataset(X_train_flat, y_train_flat, train_group_indices, seq_len=SEQ_LEN, horizon=FUTURE_LEN)
    test_dataset = LazyStockDataset(X_test_flat, y_test_flat, test_group_indices, seq_len=SEQ_LEN, horizon=FUTURE_LEN)

    # Create Loaders
    BATCH_SIZE = 64
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # Verify
    print(f"Train Loader Batches: {len(train_loader)}")
    print(f"Test Loader Batches: {len(test_loader)}")

    # Check sample
    if len(train_loader) > 0:
        x_sample, y_sample = next(iter(train_loader))
        print(f"Sample X shape: {x_sample.shape}")
        print(f"Sample y shape: {y_sample.shape}")
else:
    print("Error: Flat data arrays or indices not found. Please run the previous optimization step.")

"""## Train Frozen Model (Memory Efficient)

### Subtask:
Train the TimeSeriesTransformer using the memory-efficient LazyStockDataset and evaluate its performance.

**Reasoning**:
Initialize the TimeSeriesTransformer, loss function, and optimizer. Then, execute the training loop for 15 epochs using the lazy data loaders, calculating metrics and plotting the results as requested.
"""

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import warnings

# Suppress specific PyTorch warnings regarding nested tensors
warnings.filterwarnings("ignore", category=UserWarning)

# 1. Setup Device & Model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

input_dim = 771  # 3 price features + 768 embedding features
d_model = 32
num_heads = 4
num_layers = 4
output_dim = 1

model = TimeSeriesTransformer(
    input_dim=input_dim,
    d_model=d_model,
    num_heads=num_heads,
    num_layers=num_layers,
    output_dim=output_dim,
    dropout=0.2
).to(device)

# 2. Loss & Optimizer
criterion = nn.MSELoss()
optimizer = optim.AdamW(model.parameters(), lr=1e-3)

# 3. Training Loop
num_epochs = 15
train_losses = []
test_losses = []

print(f"Starting training for {num_epochs} epochs using Lazy Loaders...")

for epoch in range(num_epochs):
    # Train
    model.train()
    running_train_loss = 0.0
    for X_batch, y_batch in train_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)

        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()

        running_train_loss += loss.item() * X_batch.size(0)

    epoch_train_loss = running_train_loss / len(train_loader.dataset)
    train_losses.append(epoch_train_loss)

    # Validate
    model.eval()
    running_test_loss = 0.0
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            running_test_loss += loss.item() * X_batch.size(0)

    epoch_test_loss = running_test_loss / len(test_loader.dataset)
    test_losses.append(epoch_test_loss)

    print(f"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {epoch_train_loss:.6f} | Test Loss: {epoch_test_loss:.6f}")

# 4. Final Evaluation Metrics
model.eval()
all_preds = []
all_targets = []

with torch.no_grad():
    for X_batch, y_batch in test_loader:
        X_batch = X_batch.to(device)
        preds = model(X_batch).cpu().numpy()
        all_preds.append(preds)
        all_targets.append(y_batch.numpy())

all_preds = np.concatenate(all_preds).flatten()
all_targets = np.concatenate(all_targets).flatten()

mse = mean_squared_error(all_targets, all_preds)
mae = mean_absolute_error(all_targets, all_preds)
r2 = r2_score(all_targets, all_preds)

print("\nFinal Evaluation on Test Set:")
print(f"MSE: {mse:.6f}")
print(f"MAE: {mae:.6f}")
print(f"R2 Score: {r2:.6f}")

# 5. Plot Loss Curves
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Train Loss')
plt.plot(test_losses, label='Test Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.legend()

# 6. Plot Predictions (First 100 samples)
plt.subplot(1, 2, 2)
plt.plot(all_targets[:100], label='Actual', alpha=0.7)
plt.plot(all_preds[:100], label='Predicted', alpha=0.7)
plt.title('Prediction vs Actual (First 100 Test Samples)')
plt.xlabel('Sample Index')
plt.ylabel('Scaled Close Price')
plt.legend()

plt.tight_layout()
plt.show()

"""## Prepare Tokenized Text Data

### Subtask:
Process news text, align it with stock data, and generate flat tokenized arrays for training and testing.

**Reasoning**:
Process the text data by aggregating summaries, merging with stock data, imputing missing values, and performing a strict time-based split to align with the price data. Then, tokenize the text using the FinBERT tokenizer.
"""

import pandas as pd
import numpy as np
from transformers import BertTokenizer
from tqdm import tqdm
import gc
import os

# 1. Aggregate Text Data
print("Aggregating news summaries...")
# Ensure timestamp is datetime
if 'timestamp' in df_news.columns:
    df_news['timestamp'] = pd.to_datetime(df_news['timestamp']).dt.normalize()

# Group by stock and date, joining summaries
# Handle NaN in lsa_summary by filling with empty string to avoid "nan" text
df_news['lsa_summary'] = df_news['lsa_summary'].fillna("")
text_agg = df_news.groupby(['stock_symbol', 'timestamp'])['lsa_summary'].apply(lambda x: ' '.join(x.astype(str))).reset_index()
text_agg.rename(columns={'timestamp': 'date', 'lsa_summary': 'text_summary'}, inplace=True)

# 2. Merge with Stock Data
print("Merging text with stock data...")
df_stocks['date'] = pd.to_datetime(df_stocks['date']).dt.normalize()

# Left join to keep all stock rows
df_merged_text = pd.merge(df_stocks, text_agg, on=['stock_symbol', 'date'], how='left')

# 3. Impute Missing Text
print("Imputing missing text...")
# Forward fill to persist sentiment
df_merged_text['text_summary'] = df_merged_text.groupby('stock_symbol')['text_summary'].ffill()
# Fill remaining NaNs (e.g., start of history) with placeholder
df_merged_text['text_summary'] = df_merged_text['text_summary'].fillna("neutral market update")

# 4. Sort
df_merged_text = df_merged_text.sort_values(['stock_symbol', 'date']).reset_index(drop=True)

# 5. Split (Strict Time-Based) to match Price Data
print("Splitting text data...")
train_text_list = []
test_text_list = []

unique_stocks = df_merged_text['stock_symbol'].unique()

# Constants from previous steps
SEQ_LEN = 50
FUTURE_LEN = 3

for stock in unique_stocks:
    stock_df = df_merged_text[df_merged_text['stock_symbol'] == stock]
    n_rows = len(stock_df)

    # Consistency Check: Must match the filter used in generating X_train_flat
    if n_rows < (SEQ_LEN + FUTURE_LEN + 2):
        continue

    split_idx = int(n_rows * 0.8)

    train_text_list.extend(stock_df['text_summary'].iloc[:split_idx].tolist())
    test_text_list.extend(stock_df['text_summary'].iloc[split_idx:].tolist())

print(f"Train text rows: {len(train_text_list)} (Expected approx 30580)")
print(f"Test text rows: {len(test_text_list)} (Expected approx 7647)")

# 6. Tokenize
print("Tokenizing...")
tokenizer = BertTokenizer.from_pretrained("ProsusAI/finbert")

def tokenize_in_batches(text_list, batch_size=1000, max_len=32):
    input_ids_list = []
    attention_mask_list = []

    for i in tqdm(range(0, len(text_list), batch_size), desc="Tokenizing"):
        batch = text_list[i : i + batch_size]
        encoded = tokenizer(
            batch,
            padding='max_length',
            truncation=True,
            max_length=max_len,
            return_tensors='np'
        )
        input_ids_list.append(encoded['input_ids'])
        attention_mask_list.append(encoded['attention_mask'])

    if input_ids_list:
        return np.concatenate(input_ids_list, axis=0), np.concatenate(attention_mask_list, axis=0)
    else:
        return np.array([]), np.array([])

train_input_ids, train_mask = tokenize_in_batches(train_text_list)
test_input_ids, test_mask = tokenize_in_batches(test_text_list)

print(f"Train Input IDs Shape: {train_input_ids.shape}")
print(f"Test Input IDs Shape: {test_input_ids.shape}")

# Cleanup
del df_merged_text, text_agg, train_text_list, test_text_list
_ = gc.collect()

"""**Reasoning**:
I will re-run the text processing pipeline to aggregate news summaries, merge them with stock data, and tokenize the results. I'll explicitly suppress the `gc.collect()` output by assigning it to a variable and ensure missing values in `lsa_summary` are handled before aggregation.

# Task
Define the `LazyTextStockDataset` class to efficiently load data for co-finetuning. This class should accept flat arrays for price features, tokenized input IDs, attention masks, and targets, along with group indices. It must use the pre-calculated valid start indices to lazily generate sequences, returning the price sequence (length 50) and the text tokens (input IDs and attention mask) corresponding specifically to the **last day** of each sequence.

Additionally, define the `FinBERT_TimeSeries_Model` class. This model should integrate a pre-trained FinBERT encoder (`ProsusAI/finbert`) with the `TimeSeriesTransformer` architecture. The `forward` method must accept the price sequence and the last-day text tokens, generate the 768-dimensional text embedding, replicate this embedding across all time steps to match the sequence length, concatenate it with the price features, and pass the combined input to the transformer. Ensure the class includes a `freeze_finbert` parameter to control whether the BERT weights are trainable.

## Define Co-Finetuning Components

### Subtask:
Define the `LazyTextStockDataset` class for data loading and the `FinBERT_TimeSeries_Model` class for the architecture.

**Reasoning**:
Define the custom dataset class for combining text and stock data, define the model architecture integrating FinBERT and the Transformer, and then prepare the data loaders.
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertModel

# 1. Define LazyTextStockDataset
class LazyTextStockDataset(Dataset):
    def __init__(self, price_features, input_ids, attention_masks, targets, group_indices, seq_len=50, horizon=3):
        self.price_features = price_features
        self.input_ids = input_ids
        self.attention_masks = attention_masks
        self.targets = targets
        self.seq_len = seq_len
        self.horizon = horizon
        self.valid_starts = []

        # Pre-calculate valid start indices (Same logic as LazyStockDataset)
        for start, end in group_indices:
            last_valid_start = end - seq_len - horizon
            if last_valid_start >= start:
                self.valid_starts.extend(range(start, last_valid_start + 1))

    def __len__(self):
        return len(self.valid_starts)

    def __getitem__(self, idx):
        global_idx = self.valid_starts[idx]

        # 1. Slice Price Sequence
        price_seq = self.price_features[global_idx : global_idx + self.seq_len]

        # 2. Extract Text for the LAST day of the sequence
        # The sentiment at the decision point (end of sequence) is most relevant
        text_idx = global_idx + self.seq_len - 1
        input_id = self.input_ids[text_idx]
        attn_mask = self.attention_masks[text_idx]

        # 3. Get Target
        target_idx = global_idx + self.seq_len + self.horizon - 1
        y = self.targets[target_idx]

        return (
            torch.tensor(price_seq, dtype=torch.float32),
            torch.tensor(input_id, dtype=torch.long),
            torch.tensor(attn_mask, dtype=torch.long),
            torch.tensor(y, dtype=torch.float32).unsqueeze(-1)
        )

# 2. Define FinBERT_TimeSeries_Model
class FinBERT_TimeSeries_Model(nn.Module):
    def __init__(self, price_dim=3, d_model=32, num_heads=4, num_layers=4, output_dim=1, dropout=0.2, dim_feedforward=128, freeze_finbert=True):
        super(FinBERT_TimeSeries_Model, self).__init__()

        # Load Pre-trained FinBERT
        self.finbert = BertModel.from_pretrained("ProsusAI/finbert")

        if freeze_finbert:
            for param in self.finbert.parameters():
                param.requires_grad = False

        self.embedding_dim = 768
        # Input to transformer is Price Features + FinBERT Embedding
        combined_input_dim = price_dim + self.embedding_dim

        # Reuse the TimeSeriesTransformer defined earlier
        self.transformer = TimeSeriesTransformer(
            input_dim=combined_input_dim,
            d_model=d_model,
            num_heads=num_heads,
            num_layers=num_layers,
            output_dim=output_dim,
            dropout=dropout,
            dim_feedforward=dim_feedforward
        )

    def forward(self, price_seq, input_ids, attention_mask):
        # price_seq: (Batch, Seq_Len, Price_Dim)
        # input_ids: (Batch, Token_Len) -> We are only passing the text for the last step
        # attention_mask: (Batch, Token_Len)

        # 1. Get FinBERT Embedding
        finbert_out = self.finbert(input_ids=input_ids, attention_mask=attention_mask)
        cls_embedding = finbert_out.last_hidden_state[:, 0, :] # Shape: (Batch, 768)

        # 2. Replicate Embedding across the sequence
        # We assume the sentiment of the last day applies to the context of the window
        # Or essentially, we condition the whole price history on the current sentiment
        seq_len = price_seq.shape[1]
        # Expand: (Batch, 1, 768) -> (Batch, Seq_Len, 768)
        emb_expanded = cls_embedding.unsqueeze(1).expand(-1, seq_len, -1)

        # 3. Concatenate
        # Shape: (Batch, Seq_Len, Price_Dim + 768)
        combined_features = torch.cat((price_seq, emb_expanded), dim=2)

        # 4. Pass through Transformer
        output = self.transformer(combined_features)

        return output

# 3. Prepare Data and Loaders
# Slice only price features (Open, Close, Volume are the first 3 columns)
X_train_prices = X_train_flat[:, :3]
X_test_prices = X_test_flat[:, :3]

print("Creating LazyTextStockDatasets...")
train_text_dataset = LazyTextStockDataset(
    X_train_prices, train_input_ids, train_mask, y_train_flat,
    train_group_indices, seq_len=SEQ_LEN, horizon=FUTURE_LEN
)

test_text_dataset = LazyTextStockDataset(
    X_test_prices, test_input_ids, test_mask, y_test_flat,
    test_group_indices, seq_len=SEQ_LEN, horizon=FUTURE_LEN
)

# Reduced batch size for FinBERT memory usage
BATCH_SIZE = 16
train_text_loader = DataLoader(train_text_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_text_loader = DataLoader(test_text_dataset, batch_size=BATCH_SIZE, shuffle=False)

print(f"Train Text Loader Batches: {len(train_text_loader)}")
print(f"Test Text Loader Batches: {len(test_text_loader)}")