{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPhri0SPgdes"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgAF8LgRf53g"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "izt-mrk-8IEP"
      },
      "outputs": [],
      "source": [
        "!pip install awswrangler\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-Rq47-18H11"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFRGLTdcK_Lb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import awswrangler as wr\n",
        "import boto3\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from getpass import getpass\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDu2CpCJLCps"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ET_Qudws70bf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, TFBertModel, create_optimizer\n",
        "\n",
        "# Set this environment variable regarding legacy keras to be safe\n",
        "# Explanation: Transformers package uses Keras 2 objects, current version is Keras 3, packed in Tensorflow since version 2.16. Fastest fix without downgrading tensorflow is to set legacy keras usage flag as above. More info can be found here.\n",
        "# https://stackoverflow.com/questions/79309854/valueerror-exception-encountered-when-calling-layer-tf-bert-model-type-tfber\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
        "\n",
        "# import legacy keras\n",
        "import tf_keras\n",
        "from tf_keras.layers import Dense, Input\n",
        "from tf_keras.models import Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-R3czlXiJYj"
      },
      "outputs": [],
      "source": [
        "print(tf_keras.__version__)\n",
        "version_str = tf_keras.__version__\n",
        "major_version = int(version_str.split('.')[0])\n",
        "\n",
        "if major_version >= 3:\n",
        "    raise ValueError(f\"Keras version is {version_str}, which is 3.0 or higher. This setup requires Keras version < 3.0.\")\n",
        "else:\n",
        "    print(f\"Keras version {version_str} is compatible.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amy0YF5VgTkk"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3umWf2QPKIuH"
      },
      "outputs": [],
      "source": [
        "# --- IMPORTANT: Set these variables before running ---\n",
        "AWS_REGION = 'us-east-2'\n",
        "S3_STAGING_DIR = 's3://cs230-market-data-2025/athena-query-results/'\n",
        "ATHENA_DB = 'cs230_finance_data'\n",
        "# Querying more data for a small training run\n",
        "SQL_QUERY = \"SELECT concatarticles1, concatarticles2, vol_1_vs_2 FROM paired_vixy_w_titles_v3 WHERE vol_1_vs_2 is not null ORDER BY RAND() LIMIT 10000\"\n",
        "\n",
        "LABEL_COLUMN = 'vol_1_vs_2'\n",
        "# Columns:\n",
        "# concatarticles1: concatenated 10 titles (first of pair)\n",
        "# date1: articles release date (first of pair)\n",
        "# vol_diff1: percet change of volatility between tomorrow open and yesterday close\n",
        "# concatarticles2 : ditto (second or pair)\n",
        "# date2: ditto (second or pair)\n",
        "# vol_diff2 : ditto (second or pair)\n",
        "# vol_1_vs_2 : vol_diff2 - vol_diff1, paired label\n",
        "\n",
        "# --- Model & Tokenizer Configuration ---\n",
        "MODEL_NAME = \"ProsusAI/finbert\"\n",
        "MAX_LENGTH = 256\n",
        "VAL_RATIO = 0.2\n",
        "# ----------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XKn2N_agZKH"
      },
      "source": [
        "## Connect AWS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdf51ARjJ_1P"
      },
      "outputs": [],
      "source": [
        "# --- AWS Authentication for Colab ---\n",
        "# Prompt for AWS credentials\n",
        "aws_access_key_id = getpass('Enter AWS Access Key ID: ')\n",
        "aws_secret_access_key = getpass('Enter AWS Secret Access Key: ')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD4yjsjr6FTN"
      },
      "source": [
        "# Connect Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_cPRelR6G-D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "print(\"\\n--- Mounting Google Drive ---\")\n",
        "# This will prompt you to authorize access to your Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define base directory for saving models\n",
        "BASE_DRIVE_PATH = '/content/drive/MyDrive/Colab Notebooks'\n",
        "os.makedirs(BASE_DRIVE_PATH, exist_ok=True)\n",
        "\n",
        "# Define specific paths for the two models\n",
        "FULL_REGRESSION_SAVE_PATH = os.path.join(BASE_DRIVE_PATH, f'regression_model_{TIMESTAMP}.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv02zptHxS4d"
      },
      "source": [
        "## TPU config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wv5RWHLTxSOg"
      },
      "outputs": [],
      "source": [
        "# --- TPU Configuration ---\n",
        "print(\"\\n--- Step 2: Configuring TPU ---\")\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    print('Running on TPU ', tpu.master())\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except ValueError:\n",
        "    # If TPU is not available, check for GPU.\n",
        "    print('⚠️ TPU not found. Checking for GPUs.')\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "        # If GPUs are available, MirroredStrategy will use them all.\n",
        "        # If only one GPU is available, it will use that one.\n",
        "        strategy = tf.distribute.MirroredStrategy()\n",
        "        print(f'✅ Running on {len(tf.config.list_physical_devices(\"GPU\"))} GPU(s).')\n",
        "    else:\n",
        "        # If no GPU is found, fall back to CPU\n",
        "        print('⚠️ No GPUs found. Running on CPU.')\n",
        "        strategy = tf.distribute.get_strategy() # Default strategy for CPU\n",
        "\n",
        "print(f\"REPLICAS: {strategy.num_replicas_in_sync}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7Oi_3FjhUjb"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac6Gnp9IxYXB"
      },
      "source": [
        "## Read data from aws"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcl-LUs0-SLB"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "print(f\"\\n--- Step 3: Configuration set for {ATHENA_DB} ---\")\n",
        "print(f\"--- Step 4: Querying Data ---\")\n",
        "print(f\"Querying data from {ATHENA_DB}....\")\n",
        "\n",
        "# Define df in a wider scope\n",
        "df = None\n",
        "\n",
        "try:\n",
        "    # Create a boto3 session with the provided credentials\n",
        "    session = boto3.Session(\n",
        "        aws_access_key_id=aws_access_key_id,\n",
        "        aws_secret_access_key=aws_secret_access_key,\n",
        "        region_name=AWS_REGION,\n",
        "    )\n",
        "\n",
        "    # Run the query and load results into a Pandas DataFrame\n",
        "    df = wr.athena.read_sql_query(\n",
        "        sql=SQL_QUERY,\n",
        "        database=ATHENA_DB,\n",
        "        s3_output=S3_STAGING_DIR,\n",
        "        boto3_session=session,\n",
        "    )\n",
        "\n",
        "    print(\"\\nQuery successful! Data loaded into DataFrame.\")\n",
        "\n",
        "    # Display the first 5 rows\n",
        "    print(df.head())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred:\")\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c9lLaQTupCG"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhMbXbt77yCC"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------\n",
        "# Load Tokenizer\n",
        "# ---------------------------------\n",
        "print(f\"\\n--- Step 5: Loading Tokenizer ({MODEL_NAME}) ---\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# ---------------------------------\n",
        "# Preprocess & Tokenize Data\n",
        "# ---------------------------------\n",
        "print(f\"\\n--- Step 6: Tokenizing Data ---\")\n",
        "if df is not None:\n",
        "    # Separate the text columns and the label\n",
        "    text1_list = df['concatarticles1'].astype(str).tolist()\n",
        "    text2_list = df['concatarticles2'].astype(str).tolist()\n",
        "    # Ensure labels are float32 for the loss function\n",
        "    labels = df[LABEL_COLUMN].astype('float32').values\n",
        "\n",
        "    # Ensure labels are float32\n",
        "    # raw_labels = df[LABEL_COLUMN].astype('float32').values\n",
        "\n",
        "    # # --- Normalize Labels ---\n",
        "    # print(\"\\nNormalizing labels...\")\n",
        "    # LABEL_MEAN = raw_labels.mean()\n",
        "    # LABEL_STD = raw_labels.std()\n",
        "    # print(f\"Original Label Stats: Mean={LABEL_MEAN:.2f}, Std={LABEL_STD:.2f}\")\n",
        "\n",
        "    # # Standardize to Mean 0, Std 1\n",
        "    # labels = (raw_labels - LABEL_MEAN) / (LABEL_STD + 1e-8)\n",
        "    # print(f\"Normalized Label Stats: Mean={labels.mean():.2f}, Std={labels.std():.2f}\")\n",
        "\n",
        "    # Tokenize both text lists\n",
        "    print(\"Tokenizing concatarticles1...\")\n",
        "    encodings1 = tokenizer(\n",
        "        text1_list,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    print(\"Tokenizing concatarticles2...\")\n",
        "    encodings2 = tokenizer(\n",
        "        text2_list,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    # Create the input dictionary for the Keras model\n",
        "    # This format matches the Input layers we will define\n",
        "    X_train = {\n",
        "        'input_ids_1': encodings1['input_ids'],\n",
        "        'attention_mask_1': encodings1['attention_mask'],\n",
        "        'input_ids_2': encodings2['input_ids'],\n",
        "        'attention_mask_2': encodings2['attention_mask']\n",
        "    }\n",
        "    y_train = labels\n",
        "\n",
        "    print(f\"Data prepared: {len(y_train)} pairs.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nDataFrame is None. Halting script.\")\n",
        "    # In a real script, exit here\n",
        "    # sys.exit()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v6o4xLfhbJ1"
      },
      "source": [
        "## Split train / val sets\n",
        "TODO: Save splitted train / val sets in google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpsqBZ2jhdbW"
      },
      "outputs": [],
      "source": [
        "split_idx = int(len(y_train) * (1 - VAL_RATIO))\n",
        "\n",
        "print(f\"Splitting validation data from index {split_idx} to {len(y_train)}...\")\n",
        "\n",
        "# Slice the dictionary inputs for validation\n",
        "X_val = {\n",
        "    'input_ids_1': X_train['input_ids_1'][split_idx:],\n",
        "    'attention_mask_1': X_train['attention_mask_1'][split_idx:],\n",
        "    'input_ids_2': X_train['input_ids_2'][split_idx:],\n",
        "    'attention_mask_2': X_train['attention_mask_2'][split_idx:]\n",
        "}\n",
        "# Slice the labels for validation\n",
        "y_val = y_train[split_idx:]\n",
        "\n",
        "# Create training sets without the validation data\n",
        "X_train_no_val = {\n",
        "    'input_ids_1': X_train['input_ids_1'][:split_idx],\n",
        "    'attention_mask_1': X_train['attention_mask_1'][:split_idx],\n",
        "    'input_ids_2': X_train['input_ids_2'][:split_idx],\n",
        "    'attention_mask_2': X_train['attention_mask_2'][:split_idx]\n",
        "}\n",
        "y_train_no_val = y_train[:split_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe3mkZoTuzqG"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83l2MmN8iy0Z"
      },
      "source": [
        "## Check keras version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UOud9e4l2Mz"
      },
      "outputs": [],
      "source": [
        "print(tf_keras.__version__)\n",
        "version_str = tf_keras.__version__\n",
        "major_version = int(version_str.split('.')[0])\n",
        "\n",
        "if major_version >= 3:\n",
        "    raise ValueError(f\"Keras version is {version_str}, which is 3.0 or higher. This setup requires Keras version < 3.0.\")\n",
        "else:\n",
        "    print(f\"Keras version {version_str} is compatible.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grosU-XiivMX"
      },
      "source": [
        "## Hyper params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HP9EU5wqinlU"
      },
      "outputs": [],
      "source": [
        "num_layers_to_unfreeze = 2\n",
        "\n",
        "# Set up epochs and steps\n",
        "epochs = 1\n",
        "batch_size = 16 # Set this so that it fits on the GPU\n",
        "\n",
        "# Correctly calculate train data size using the labels array\n",
        "train_data_size = len(y_train_no_val)\n",
        "steps_per_epoch = int(train_data_size / batch_size)\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzoQ1Rd-i-lo"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "S0FPn35Duyw1"
      },
      "outputs": [],
      "source": [
        "def create_regression_model(max_len=256):\n",
        "    # Load the model FRESH every time we create it to avoid stale state\n",
        "    bert_only = TFBertModel.from_pretrained(\"ProsusAI/finbert\", from_pt=True)\n",
        "\n",
        "    # --- Freeze Layers ---\n",
        "    bert_only.trainable = True\n",
        "\n",
        "    num_bert_layers = len(bert_only.bert.encoder.layer)\n",
        "    num_layers_to_freeze = num_bert_layers - num_layers_to_unfreeze\n",
        "\n",
        "    for i, layer in enumerate(bert_only.bert.encoder.layer):\n",
        "        if i < num_layers_to_freeze:\n",
        "            layer.trainable = False\n",
        "        else:\n",
        "            layer.trainable = True\n",
        "\n",
        "    # --- ADD INPUT HERE (Using tf_keras) ---\n",
        "    input_ids_1 = Input(shape=(max_len,), dtype=tf.int32, name='input_ids_1')\n",
        "    attention_mask_1 = Input(shape=(max_len,), dtype=tf.int32, name='attention_mask_1')\n",
        "\n",
        "    input_ids_2 = Input(shape=(max_len,), dtype=tf.int32, name='input_ids_2')\n",
        "    attention_mask_2 = Input(shape=(max_len,), dtype=tf.int32, name='attention_mask_2')\n",
        "    # --- Tower 1 ---\n",
        "    output_1 = bert_only(input_ids=input_ids_1, attention_mask=attention_mask_1)\n",
        "    # --- Tower 2 ---\n",
        "    output_2 = bert_only(input_ids=input_ids_2, attention_mask=attention_mask_2)\n",
        "    embedding_1 = output_1.pooler_output\n",
        "    embedding_2 = output_2.pooler_output\n",
        "\n",
        "    concatenated_embeddings = tf.concat([embedding_1, embedding_2], axis=1)\n",
        "\n",
        "    # x = Dense(768, activation='relu')(concatenated_embeddings)\n",
        "    # Add a final regression/classification head\n",
        "    output = Dense(1, activation='linear')(concatenated_embeddings)\n",
        "\n",
        "    # Build model using tf_keras.models.Model\n",
        "    model = Model(inputs=[input_ids_1, attention_mask_1, input_ids_2, attention_mask_2], outputs=output)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Instantiate and check\n",
        "model = create_regression_model(max_len=256)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEeClQvRUCdY"
      },
      "source": [
        "## Optimizer & Compile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CGFwYclUCdY"
      },
      "outputs": [],
      "source": [
        "## Creates an optimizer with learning rate schedule, using warmup steps and\n",
        "## weight decay (AdamWeightDecay)\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)\n",
        "\n",
        "## Use sparse when the classes are not one hot encoded\n",
        "# metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
        "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False) # False when the output is a probability, like when using softmax\n",
        "model.compile(\n",
        "        loss=tf.keras.losses.MeanSquaredError(),\n",
        "        optimizer=optimizer\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTWfYyuUUCdY"
      },
      "source": [
        "## Optional: save init weights of bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQ8bCkHoUCdY"
      },
      "outputs": [],
      "source": [
        "# 1. Initialize an empty global dictionary\n",
        "initial_bert_weights_dict = {}\n",
        "\n",
        "print(\"\\n--- Storing Initial BERT Encoder Layer Weights ---\")\n",
        "\n",
        "# Find the BERT layer in the Functional model\n",
        "# We look for the layer that is the TFBertModel (usually contains 'bert' in name)\n",
        "bert_layer_object = None\n",
        "for layer in model.layers:\n",
        "    if 'bert' in layer.name:\n",
        "        bert_layer_object = layer\n",
        "        break\n",
        "\n",
        "if bert_layer_object:\n",
        "    # 2. Iterate through each layer of the BERT encoder\n",
        "    # Structure: TFBertModel -> .bert (MainLayer) -> .encoder -> .layer (list of TFBertLayer)\n",
        "    for i, encoder_layer in enumerate(bert_layer_object.bert.encoder.layer):\n",
        "        # 3. Check if encoder_layer.trainable_variables is not empty\n",
        "        if len(encoder_layer.trainable_variables) > 0:\n",
        "            # 4. Store the numpy() array of the first trainable variable\n",
        "            initial_bert_weights_dict[i] = encoder_layer.trainable_variables[0].numpy().copy()\n",
        "            print(f\"Stored initial weights for BERT Layer {i}: {encoder_layer.trainable_variables[0].name}\")\n",
        "            print(f\"BERT layer samples (first 5 values): {initial_bert_weights_dict[i].flatten()[:5]}\")\n",
        "        else:\n",
        "            print(f\"BERT Layer {i} has no trainable variables.\")\n",
        "else:\n",
        "    print(\"Error: Could not find a BERT layer in the model.\")\n",
        "\n",
        "# 5. Print a message confirming that the initial weights for each relevant BERT layer have been stored.\n",
        "print(f\"\\nInitial weights for {len(initial_bert_weights_dict)} BERT encoder layers stored in 'initial_bert_weights_dict'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSH16lnfjdEG"
      },
      "source": [
        "## Optional: Re-init, in case of re-train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ER0S6ZxHUCdZ"
      },
      "outputs": [],
      "source": [
        "# Re-initialize model to be sure\n",
        "model = create_regression_model(max_len=256)\n",
        "# model = create_regression_model(max_len=256)\n",
        "\n",
        "# Use tf_keras.optimizers.Adam to match the tf_keras model\n",
        "optimizer = tf_keras.optimizers.Adam(learning_rate=2e-5)\n",
        "\n",
        "model.compile(\n",
        "        loss=tf.keras.losses.MeanSquaredError(),\n",
        "        optimizer=optimizer\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqv1RY-76DPx"
      },
      "source": [
        "##Create Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDbTYBRm1XHb"
      },
      "outputs": [],
      "source": [
        "# Create checkpoints\n",
        "# Checkpoint directory\n",
        "CHECKPOINT_DIR = os.path.join(BASE_DRIVE_PATH, 'training_checkpoints')\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "checkpoint_path = os.path.join(CHECKPOINT_DIR, 'cp-{epoch:04d}.ckpt')\n",
        "\n",
        "# Create checkpoints\n",
        "cp_callback = tf_keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_weights_only=True,  # Set to False to save the entire model (architecture + weights + optimizer state)\n",
        "        verbose=1,               # Set to 0 for silent, 1 for progress bar, 2 for one line per epoch\n",
        "        save_best_only=True,     # Save only the best model based on monitor\n",
        "        monitor='val_loss',      # Metric to monitor (e.g., 'val_loss', 'val_accuracy')\n",
        "        mode='min',              # 'min' for metrics like loss, 'max' for metrics like accuracy\n",
        "        save_freq='epoch'        # 'epoch' to save after each epoch, or an integer for number of batches\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfH6pJlamSMm"
      },
      "source": [
        "## Train the model\n",
        "TODO: Optionally, load the weights from a checkpoint. Use this when Colab kicked you off the vm and a checkpoint was saved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXUu0Sp9WT0c"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "        X_train_no_val,\n",
        "        y_train_no_val,\n",
        "        epochs=3,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(X_val, y_val),\n",
        "        callbacks=[cp_callback]\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwdoUtAeUCdZ"
      },
      "source": [
        "##  Optional: compare bert weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv4b7nB_UCdZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "print(\"\\n--- Analyzing BERT Encoder Layer Weights ---\\n\")\n",
        "\n",
        "# Find the BERT layer in the Functional model\n",
        "bert_layer_object = None\n",
        "for layer in model.layers:\n",
        "    if 'bert' in layer.name:\n",
        "        bert_layer_object = layer\n",
        "        break\n",
        "\n",
        "if bert_layer_object:\n",
        "    # Iterate through all BERT encoder layers and compare initial vs. trained weights\n",
        "    for i, bert_layer_trained in enumerate(bert_layer_object.bert.encoder.layer):\n",
        "        # Check if this layer had initial weights stored and has trainable variables now\n",
        "        if i in initial_bert_weights_dict and len(bert_layer_trained.trainable_variables) > 0:\n",
        "            initial_weights_for_layer = initial_bert_weights_dict[i]\n",
        "            # Get the first trainable variable (usually the query kernel)\n",
        "            trained_weights_for_layer = bert_layer_trained.trainable_variables[0].numpy().copy()\n",
        "\n",
        "            # Ensure shapes are compatible for comparison\n",
        "            if initial_weights_for_layer.shape == trained_weights_for_layer.shape:\n",
        "                print(f\"Init BERT layer samples (first 5 values): {initial_weights_for_layer.flatten()[:5]}\")\n",
        "                print(f\"Trained BERT layer samples (first 5 values): {trained_weights_for_layer.flatten()[:5]}\")\n",
        "                non_identical_weights_count = np.sum(initial_weights_for_layer != trained_weights_for_layer)\n",
        "                total_weights_in_variable = initial_weights_for_layer.size\n",
        "\n",
        "                variable_name = bert_layer_trained.trainable_variables[0].name\n",
        "\n",
        "                print(f\"Layer {i} (Variable: {variable_name}):\")\n",
        "                print(f\"  Total weights in this variable: {total_weights_in_variable}\")\n",
        "                print(f\"  Non-identical weights after training: {non_identical_weights_count}\")\n",
        "                if total_weights_in_variable > 0:\n",
        "                    print(f\"  Percentage of non-identical weights: { (non_identical_weights_count / total_weights_in_variable) * 100:.2f}%\\n\")\n",
        "                else:\n",
        "                    print(\"  (No weights to compare in this variable)\\n\")\n",
        "            else:\n",
        "                print(f\"Error: Shape mismatch for Layer {i} during comparison.\\n\")\n",
        "        elif i in initial_bert_weights_dict and len(bert_layer_trained.trainable_variables) == 0:\n",
        "            print(f\"Layer {i}: Had initial weights, but no trainable variables after model build/freeze check. (Likely frozen)\\n\")\n",
        "        else:\n",
        "            print(f\"Layer {i}: No initial weights stored or no trainable variables to compare.\\n\")\n",
        "\n",
        "    print(\"\\n--- Overall Trainable Parameters for BERT Encoder ---\")\n",
        "    # Calculate total trainable params for the BERT layer\n",
        "    overall_bert_trainable_params = np.sum([np.prod(v.shape) for v in bert_layer_object.trainable_variables])\n",
        "    print(f\"Total trainable parameters in BERT Encoder: {overall_bert_trainable_params}\")\n",
        "else:\n",
        "    print(\"Error: Could not find a BERT layer in the model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufBUpj4mnL3T"
      },
      "source": [
        "## Reload checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxVkRuTcnLFB"
      },
      "outputs": [],
      "source": [
        "# --- Reload and Verify Checkpoint ---\n",
        "print(\"\\n--- Reloading best checkpoint to verify ---\")\n",
        "# Find the latest checkpoint in the directory\n",
        "latest = tf.train.latest_checkpoint(CHECKPOINT_DIR)\n",
        "\n",
        "if latest:\n",
        "    print(f\"Found checkpoint: {latest}\")\n",
        "    # Load the weights\n",
        "    model.load_weights(latest)\n",
        "    print(\"Weights loaded. Evaluating on validation set...\")\n",
        "\n",
        "    # Evaluate to check if the loss matches the best val_loss from training\n",
        "    val_loss = model.evaluate(X_val, y_val, batch_size=batch_size, verbose=1)\n",
        "    print(f\"Validation Loss from loaded checkpoint: {val_loss}\")\n",
        "else:\n",
        "    print(\"No checkpoint found to reload.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AeXQEjiswo4"
      },
      "source": [
        "# Evaluation: Outdated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6fuXD_9w5pz"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVU81oBQsvB5"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if df is not None:\n",
        "\n",
        "   # ---------------------------------\n",
        "   # 2. Generate Regression Predictions\n",
        "   # ---------------------------------\n",
        "   print(\"Running predictions on validation set...\")\n",
        "   # The regression model outputs a continuous value\n",
        "   val_raw_predictions = model.predict(X_val, batch_size=128, verbose=1)\n",
        "   val_raw_predictions = val_raw_predictions.flatten()\n",
        "\n",
        "   # ---------------------------------\n",
        "   # 3. Apply a fixed threshold for Binary Classification\n",
        "   # ---------------------------------\n",
        "   # As requested, skipping optimal threshold search and using a fixed threshold.\n",
        "   best_threshold = 0.5 # Default threshold for binary classification\n",
        "\n",
        "   print(f\"\\nUsing a fixed classification threshold: {best_threshold:.2f}\")\n",
        "\n",
        "   # ---------------------------------\n",
        "   # 4. Final Evaluation Metrics\n",
        "   # ---------------------------------\n",
        "   # Generate final class predictions using the fixed threshold\n",
        "   val_predictions = (val_raw_predictions > best_threshold).astype(np.float32)\n",
        "   final_accuracy = accuracy_score(y_val, val_predictions)\n",
        "   final_f1 = f1_score(y_val, val_predictions, zero_division=0)\n",
        "\n",
        "   print(\"\\n--- Final Validation Performance Report ---\")\n",
        "   print(f\"Threshold used: {best_threshold:.2f}\")\n",
        "   print(f\"Final Accuracy: {final_accuracy:.2f}\")\n",
        "   print(f\"Final F1 Score: {final_f1:.4f}\")\n",
        "   print(\"\\nConfusion Matrix:\")\n",
        "   # Format: [[TN, FP], [FN, TP]]\n",
        "   print(confusion_matrix(y_val, val_predictions))\n",
        "\n",
        "   print(\"\\nClassification Report:\")\n",
        "   print(classification_report(y_val, val_predictions, target_names=['Dissimilar (0)', 'Similar (1)']))\n",
        "\n",
        "else:\n",
        "   print(\"Cannot evaluate: DataFrame was not loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm9NcwGiz-9m"
      },
      "source": [
        "# Embedding Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1_wrtme0JZ8"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Creating Embedding Extraction Model ---\")\n",
        "\n",
        "# 1. Define input layers for each text input\n",
        "input_ids_1_embed = tf_keras.Input(shape=(MAX_LENGTH,), dtype=tf.int32, name='input_ids_1_embed')\n",
        "attention_mask_1_embed = tf_keras.Input(shape=(MAX_LENGTH,), dtype=tf.int32, name='attention_mask_1_embed')\n",
        "\n",
        "input_ids_2_embed = tf_keras.Input(shape=(MAX_LENGTH,), dtype=tf.int32, name='input_ids_2_embed')\n",
        "attention_mask_2_embed = tf_keras.Input(shape=(MAX_LENGTH,), dtype=tf.int32, name='attention_mask_2_embed')\n",
        "\n",
        "# 2. Extract the fine-tuned TFBertModel layer from the already trained `model`\n",
        "bert_layer = None\n",
        "for layer in model.layers:\n",
        "    if 'tf_bert_model' in layer.name:\n",
        "        bert_layer = layer\n",
        "        break\n",
        "\n",
        "if bert_layer is None:\n",
        "    raise ValueError(\"Could not find the TFBertModel layer in the trained model.\")\n",
        "\n",
        "# 3. Pass the defined input layers through the extracted bert_layer\n",
        "# Only output the pooler_output for embedding extraction for both texts\n",
        "embedding_output_1 = bert_layer(input_ids=input_ids_1_embed, attention_mask=attention_mask_1_embed).pooler_output\n",
        "embedding_output_2 = bert_layer(input_ids=input_ids_2_embed, attention_mask=attention_mask_2_embed).pooler_output\n",
        "\n",
        "# 4. Create a new tf_keras.models.Model named `embedding_model`\n",
        "# This model takes both sets of inputs and outputs both sets of embeddings\n",
        "embedding_model = Model(\n",
        "    inputs=[\n",
        "        input_ids_1_embed, attention_mask_1_embed,\n",
        "        input_ids_2_embed, attention_mask_2_embed\n",
        "    ],\n",
        "    outputs=[embedding_output_1, embedding_output_2]\n",
        ")\n",
        "\n",
        "print(\"Embedding extraction model created successfully.\")\n",
        "embedding_model.summary()\n",
        "\n",
        "print(\"\\n--- Extracting Embeddings ---\")\n",
        "# Extract embeddings for the training data\n",
        "embeddings_1, embeddings_2 = embedding_model.predict(\n",
        "    [X_train_no_val['input_ids_1'], X_train_no_val['attention_mask_1'],\n",
        "     X_train_no_val['input_ids_2'], X_train_no_val['attention_mask_2']],\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "print(f\"Extracted embeddings for concatarticles1: {embeddings_1.shape}\")\n",
        "print(f\"Extracted embeddings for concatarticles2: {embeddings_2.shape}\")\n",
        "\n",
        "# --- Saving Embeddings ---\n",
        "print(\"\\n--- Saving Embeddings to NumPy files ---\")\n",
        "\n",
        "# Create a directory to save embeddings if it doesn't exist\n",
        "EMBEDDING_SAVE_DIR = os.path.join(BASE_DRIVE_PATH, 'embeddings')\n",
        "os.makedirs(EMBEDDING_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Define file paths\n",
        "embeddings_1_path = os.path.join(EMBEDDING_SAVE_DIR, \"embeddings_concatarticles1_train.npy\")\n",
        "embeddings_2_path = os.path.join(EMBEDDING_SAVE_DIR, \"embeddings_concatarticles2_train.npy\")\n",
        "\n",
        "# Save the embeddings\n",
        "np.save(embeddings_1_path, embeddings_1)\n",
        "np.save(embeddings_2_path, embeddings_2)\n",
        "\n",
        "print(f\"Embeddings for concatarticles1 saved to: {embeddings_1_path}\")\n",
        "print(f\"Embeddings for concatarticles2 saved to: {embeddings_2_path}\")\n",
        "print(\"Embedding extraction and saving complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzMrqk3vFLSk"
      },
      "source": [
        "## Load embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVyHJ9igFYiw"
      },
      "outputs": [],
      "source": [
        "loaded_embeddings_1 = np.load(embeddings_1_path)\n",
        "loaded_embeddings_2 = np.load(embeddings_2_path)\n",
        "\n",
        "print(\"Embeddings loaded successfully!\")\n",
        "print(f\"Shape for loaded_embeddings_1: {loaded_embeddings_1.shape}\")\n",
        "print(f\"Shape for loaded_embeddings_2: {loaded_embeddings_2.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-upZsGfkvH1c"
      },
      "source": [
        "# Load & Re-evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPQboIFbNXYl"
      },
      "outputs": [],
      "source": [
        "# --- Loading the saved model with weight transfer ---\n",
        "print(\"\\n--- Loading the saved model via weight transfer ---\")\n",
        "\n",
        "# 1. Instantiate a FRESH, un-trained version of the model structure.\n",
        "# This runs the __init__ but only creates the necessary layers (TFBertModel, Dense).\n",
        "# We MUST use the MODEL_NAME in the __init__ to correctly configure the TFBertModel.\n",
        "try:\n",
        "    # Instantiate the model with the same MODEL_NAME used during training\n",
        "    fresh_regression_model = MarketDiffRegressor(MODEL_NAME)\n",
        "\n",
        "    # 2. You must call the model once to build its tensors before loading weights.\n",
        "    # We'll use a single sample from your prepared validation data (X_val).\n",
        "    _ = fresh_regression_model(\n",
        "        {\n",
        "            'input_ids_1': X_val['input_ids_1'][:1],\n",
        "            'attention_mask_1': X_val['attention_mask_1'][:1],\n",
        "            'input_ids_2': X_val['input_ids_2'][:1],\n",
        "            'attention_mask_2': X_val['attention_mask_2'][:1]\n",
        "        }\n",
        "    )\n",
        "    print(\"Fresh model structure built.\")\n",
        "\n",
        "    # 3. Load the weights directly from the saved file.\n",
        "    # This bypasses the problematic tf.keras.models.load_model and ensures\n",
        "    # only the fine-tuned weights are applied to the built structure.\n",
        "    fresh_regression_model.load_weights(FULL_REGRESSION_SAVE_PATH)\n",
        "    loaded_regression_model = fresh_regression_model # Use this for predictions\n",
        "    print(f\"Fine-tuned weights loaded successfully from: {FULL_REGRESSION_SAVE_PATH}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during weight loading: {e}\")\n",
        "    # Handle the error if weights cannot be loaded\n",
        "\n",
        "# --- Re-evaluating with the loaded model ---\n",
        "print(\"\\n--- Re-evaluating with the loaded model ---\")\n",
        "\n",
        "if df is not None:\n",
        "    # Generate Regression Predictions using the loaded model\n",
        "    print(\"Running predictions on validation set with loaded model...\")\n",
        "    # Using the 'loaded_regression_model' (which is fresh_regression_model with loaded weights)\n",
        "    loaded_val_raw_predictions = loaded_regression_model.predict(X_val, batch_size=128, verbose=1)\n",
        "    loaded_val_raw_predictions = loaded_val_raw_predictions.flatten()\n",
        "\n",
        "    # Apply the same fixed threshold for Binary Classification\n",
        "    print(f\"\\nUsing a fixed classification threshold: {best_threshold:.2f}\")\n",
        "\n",
        "    # Generate final class predictions using the fixed threshold\n",
        "    loaded_val_predictions = (loaded_val_raw_predictions > best_threshold).astype(np.float32)\n",
        "\n",
        "    # Final Evaluation Metrics\n",
        "    loaded_final_accuracy = accuracy_score(y_val, loaded_val_predictions)\n",
        "    loaded_final_f1 = f1_score(y_val, loaded_val_predictions, zero_division=0)\n",
        "\n",
        "    print(\"\\n--- Evaluation Report for Loaded Model ---\")\n",
        "    print(f\"Threshold used: {best_threshold:.2f}\")\n",
        "    print(f\"Final Accuracy: {loaded_final_accuracy:.2f}\")\n",
        "    print(f\"Final F1 Score: {loaded_final_f1:.4f}\")\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_val, loaded_val_predictions))\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_val, loaded_val_predictions, target_names=['Dissimilar (0)', 'Similar (1)']))\n",
        "else:\n",
        "    print(\"Cannot evaluate: DataFrame was not loaded.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "0AeXQEjiswo4"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
